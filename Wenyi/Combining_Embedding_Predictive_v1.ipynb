{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.6.5 |Anaconda, Inc.| (default, Apr 26 2018, 08:42:37) \n",
      "[GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)]\n",
      "Numpy version: 1.14.3\n",
      "Pandas version: 0.23.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "pd.options.display.max_rows = 99\n",
    "pd.options.display.max_columns = 99\n",
    "\n",
    "print('Python version: '+sys.version)\n",
    "print('Numpy version: '+np.__version__)\n",
    "print('Pandas version: '+pd.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#import statsmodels.api as sm\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import os, sys, time\n",
    "from importlib import reload\n",
    "path=os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train sample shape: (18463, 49)\n",
      "test sample shape: (19214, 49)\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv('sample_train.csv')\n",
    "df_train.drop(labels=['Unnamed: 0'], axis=1, inplace=True)\n",
    "print('train sample shape: '+str(df_train.shape))\n",
    "df_test = pd.read_csv('sample_test.csv')\n",
    "df_test.drop(labels=['Unnamed: 0'], axis=1, inplace=True)\n",
    "print('test sample shape: '+str(df_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Embedding, Concatenate, Reshape, BatchNormalization, LSTM, CuDNNLSTM, CuDNNGRU, Lambda\n",
    "from keras.models import Model\n",
    "from keras.layers.core import Dense, Activation, Dropout, RepeatVector\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import Adam\n",
    "from keras.initializers import Constant\n",
    "import keras.backend as K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Embedding, Conv2D, MaxPool2D\n",
    "from keras.layers import Reshape, Flatten, Dropout, Concatenate\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "#from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_DX = 29\n",
    "n_PR = 15\n",
    "DXs = ['DX'+str(n) for n in range(2,n_DX+2)]\n",
    "PRs = ['PR'+str(n) for n in range(1,n_PR+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data\n",
    "df_train['y'] = 0\n",
    "df_train.loc[df_train['readm30']==True, 'y'] = 1\n",
    "df_test['y'] = 0\n",
    "df_test.loc[df_test['readm30']==True, 'y'] = 1\n",
    "\n",
    "X_cols = ['DX1', 'DX2', 'DX3', 'DX4', 'DX5', 'DX6', 'DX7', 'DX8', 'DX9', 'DX10',\n",
    "          'DX11', 'DX12', 'DX13', 'DX14', 'DX15', 'DX16', 'DX17', 'DX18', 'DX19',\n",
    "          'DX20', 'DX21', 'DX22', 'DX23', 'DX24', 'DX25', 'DX26', 'DX27', 'DX28',\n",
    "          'DX29', 'DX30', 'PR1', 'PR2', 'PR3', 'PR4', 'PR5', 'PR6', 'PR7', 'PR8',\n",
    "          'PR9', 'PR10', 'PR11', 'PR12', 'PR13', 'PR14', 'PR15', 'AGE', 'FEMALE', 'HOSP_NRD']\n",
    "\n",
    "X_train = df_train[X_cols].values.astype(np.int)\n",
    "Y_train = df_train[['y']].values.astype(np.int)\n",
    "X_test = df_test[X_cols].values.astype(np.int)\n",
    "Y_test = df_test[['y']].values.astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from setsum_layer import SetSum, MaskedSum, MaskedDense, MaskedPooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_embed_dim = 300\n",
    "md_width = 5\n",
    "fc_width = 512\n",
    "hosp_embed_dim = 1\n",
    "dropout = 0.3\n",
    "job_index = 0\n",
    "batchsize = 28\n",
    "n_fold = 5\n",
    "n_samples=100\n",
    "n_disease=2135"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_mat=np.random.uniform(0,1,size=[2135,code_embed_dim])\n",
    "embed_initializer = Constant(np.random.uniform(0,1,size=[2135,code_embed_dim]))\n",
    "skf = StratifiedKFold(n_splits=n_fold, random_state=24, shuffle=True)\n",
    "trn_idx, val_idx = next(skf.split(X_train,Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df=df_train\n",
    "DX1_array_train = train_df['DX1'].values\n",
    "            \n",
    "DX1_array_trn = DX1_array_train[trn_idx]\n",
    "DX1_array_val = DX1_array_train[val_idx]\n",
    "\n",
    "DX_mat_train = train_df[DXs].values\n",
    "            \n",
    "DX_mat_trn = DX_mat_train[trn_idx, :]\n",
    "DX_mat_val = DX_mat_train[val_idx, :]\n",
    "\n",
    "PR_mat_train = train_df[PRs].values\n",
    "            \n",
    "PR_mat_trn = PR_mat_train[trn_idx, :]\n",
    "PR_mat_val = PR_mat_train[val_idx, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "hosp_array_train = train_df['HOSP_NRD'].values\n",
    "    \n",
    "hosp_array_trn = hosp_array_train[trn_idx]\n",
    "hosp_array_val = hosp_array_train[val_idx]\n",
    "age_mean = train_df['AGE'].mean()\n",
    "age_std = train_df['AGE'].std()\n",
    "demo_mat_train = train_df[['AGE', 'FEMALE']].values\n",
    "demo_mat_train[:, 0] = (demo_mat_train[:, 0]-age_mean)/age_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_mat_train = demo_mat_train\n",
    "\n",
    "other_mat_trn = other_mat_train[trn_idx, :]\n",
    "other_mat_val = other_mat_train[val_idx, :]\n",
    "\n",
    "y_train = train_df.readm30.astype(int).values\n",
    "Y_trn = to_categorical(y_train[trn_idx])\n",
    "Y_val = to_categorical(y_train[val_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'setsum_nn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "numvar=45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "hosp_cat = set(df_train['HOSP_NRD']).union(set(df_test['HOSP_NRD']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "####new one\n",
    "from keras.engine.topology import Layer\n",
    "from keras import objectives\n",
    "\n",
    "def zero_loss(y_true, y_pred):\n",
    "    return K.zeros_like(y_pred)\n",
    "\n",
    "\n",
    "class CustomRegularization(Layer):\n",
    "    def __init__(self, num_samples,num_classes, **kwargs):\n",
    "        self.num_samples = num_samples\n",
    "        self.num_classes = num_classes\n",
    "        super(CustomRegularization, self).__init__(**kwargs)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        dense_shape, classes_shape = input_shape\n",
    "        self.kernel = self.add_weight(name='kernel',\n",
    "                                      shape=(self.num_classes, (dense_shape[2]-1)),\n",
    "                                      initializer='uniform',\n",
    "                                      trainable=True)\n",
    "        self.bias = self.add_weight(name='bias',\n",
    "                                    shape=(self.num_classes,),\n",
    "                                    initializer='uniform',\n",
    "                                    trainable=True) \n",
    "        super(CustomRegularization, self).build(input_shape)\n",
    "\n",
    "    def call(self,x):\n",
    "        inputs_in, labels_in = x\n",
    "        #weights num_disease*num_embedding -> num_disease*(num_embedding+1) the last column is inf/0.0001\n",
    "        tempt = np.ones([self.num_classes,1])/10000\n",
    "        tempt[0,0] = -np.inf\n",
    "        tempt = tf.cast(tf.constant(tempt),tf.float32)\n",
    "        weights = Concatenate(axis=1)([self.kernel,tempt])\n",
    "        # gather input to batch_size*num_embedding\n",
    "        # gather labels to batch_size*1\n",
    "        inputs = tf.gather(inputs_in,0,axis=1)\n",
    "        labels = tf.gather(labels_in,[1],axis=1)\n",
    "        #for each context and center world, calculate posterior probability\n",
    "        # to initialize loss since its batch_size*1 which cann't be initialized directly\n",
    "        loss = tf.nn.sampled_softmax_loss(\n",
    "            weights = weights,\n",
    "            biases = self.bias,\n",
    "            labels = labels,\n",
    "            inputs = inputs,\n",
    "            num_sampled=self.num_samples,\n",
    "            num_classes=self.num_classes,\n",
    "            num_true=1)\n",
    "        for j in range(2,45):\n",
    "            labels = tf.gather(labels_in,[j],axis=1)\n",
    "            loss += tf.nn.sampled_softmax_loss(\n",
    "                weights = weights,\n",
    "                biases = self.bias,\n",
    "                labels = labels,\n",
    "                inputs = inputs,\n",
    "                num_sampled=self.num_samples,\n",
    "                num_classes=self.num_classes,\n",
    "                num_true=1)\n",
    "        for i in range(1,45):\n",
    "            inputs = tf.gather(inputs_in,0,axis=1)\n",
    "            for j in range(45):\n",
    "                if i!=j:\n",
    "                    loss+=tf.nn.sampled_softmax_loss(\n",
    "                        weights = weights,\n",
    "                        biases = self.bias,\n",
    "                        labels = labels,\n",
    "                        inputs = inputs,\n",
    "                        num_sampled=self.num_samples,\n",
    "                        num_classes=self.num_classes,\n",
    "                        num_true=1)\n",
    "        cost = tf.reduce_mean(loss)\n",
    "        self.add_loss(cost,x)\n",
    "        #you can output whatever you need, just update output_shape adequately\n",
    "        #But this is probably useful\n",
    "        return cost\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        dense_shape,classes_shape = input_shape\n",
    "        return (dense_shape[0],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_DX1 = Input(shape=(1,))\n",
    "DX1_embed0 = Embedding(input_dim=embed_mat.shape[0], output_dim=code_embed_dim, embeddings_initializer=embed_initializer, \n",
    "                      name='DX1_embed')(input_DX1)\n",
    "DX1_embed = Reshape((code_embed_dim,))(DX1_embed0)\n",
    "input_DX = Input(shape = (len(DXs),))\n",
    "DX_embed0 = Embedding(input_dim=embed_mat.shape[0], output_dim=code_embed_dim, mask_zero=False, embeddings_initializer=embed_initializer, \n",
    "                     name='DX_embed')(input_DX)\n",
    "if model_name=='setsum_nn' or 'setsum_lr':\n",
    "    DX_embed = MaskedDense(md_width, activation='relu')(DX_embed0)\n",
    "    DX_embed = MaskedSum()(DX_embed0)\n",
    "elif model_name=='embed_sum':\n",
    "    DX_embed = MaskedSum()(DX_embed0)\n",
    "elif model_name=='embed_pool':\n",
    "    DX_embed = MaskedPooling()(DX_embed0)\n",
    "input_PR = Input(shape = (len(PRs),))\n",
    "PR_embed0 = Embedding(input_dim=embed_mat.shape[0], output_dim=code_embed_dim, mask_zero=False, embeddings_initializer=embed_initializer, \n",
    "                     name='PR_embed')(input_PR)\n",
    "if model_name=='setsum_nn' or 'setsum_lr':\n",
    "    PR_embed = MaskedDense(md_width, activation='relu')(PR_embed0)\n",
    "    PR_embed = MaskedSum()(PR_embed0)\n",
    "elif model_name=='embed_sum':\n",
    "    PR_embed = MaskedSum()(PR_embed0)\n",
    "elif model_name=='embed_pool':\n",
    "    PR_embed = MaskedPooling()(PR_embed0)\n",
    "input_hosp = Input(shape=(1,))\n",
    "hosp_embed = Embedding(input_dim=len(hosp_cat), output_dim=hosp_embed_dim, input_length=1)(input_hosp)\n",
    "hosp_embed = Reshape((hosp_embed_dim, ))(hosp_embed)\n",
    "input_other = Input(shape=(other_mat_train.shape[1], ))\n",
    "merged = Concatenate(axis=1)([DX1_embed, DX_embed, PR_embed, hosp_embed, input_other])\n",
    "merged1 = Concatenate(axis=1)([DX1_embed0, DX_embed0, PR_embed0])\n",
    "#merged1 tensor*45*(num_embedding+1)\n",
    "merged2 = Concatenate(axis=1)([input_DX1,input_DX,input_PR])\n",
    "merged2 = Reshape((45,))(merged2)\n",
    "#merged2 tensor*45\n",
    "cr =  CustomRegularization(n_samples,n_disease)([merged1,merged2])\n",
    "if model_name=='setsum_nn':\n",
    "    merged = Dense(fc_width, activation='relu')(merged)\n",
    "    merged = Dropout(dropout)(merged)\n",
    "prediction = Dense(2, activation='softmax')(merged)\n",
    "model = Model(inputs=[input_DX1, input_DX, input_PR, input_hosp, input_other], outputs=[prediction,cr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in model.layers:\n",
    "    if l.name=='DX_embed' or l.name=='PR_embed':\n",
    "        l.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = Adam(lr=0.0002)\n",
    "model.compile(optimizer=adam, loss=[K.categorical_crossentropy,zero_loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14770 samples, validate on 3693 samples\n",
      "Epoch 1/10\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit([DX1_array_trn, DX_mat_trn, PR_mat_trn, hosp_array_trn, other_mat_trn], [Y_trn,np.random.randn(Y_trn.shape[0],1)], \n",
    "                     batch_size=batchsize, epochs=10, \n",
    "                     validation_data=[[DX1_array_val, DX_mat_val, PR_mat_val, hosp_array_val, other_mat_val], [Y_val,np.random.randn(Y_val.shape[0],1)]], \n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
