{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import os, sys\n",
    "import itertools\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, auc, average_precision_score\n",
    "import statsmodels.stats.api as sms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/nfs/turbo/umms-awaljee/wsliu/Data/NRD/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_path = '/home/wsliu/Codes/DLproj'\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "if module_path+'/NRD' not in sys.path:\n",
    "    sys.path.append(module_path+'/NRD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ccs_tools import core_dtypes_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from utils import preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort = 'ami'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = pd.read_csv(path+'cohorts30/{}/pred_comorb.csv'.format(cohort), dtype=core_dtypes_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_df = pd.read_csv(path+'cohorts30/{}/index_comorb.csv'.format(cohort), dtype=core_dtypes_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_csv(path+'cohorts/ami/ami_pred.csv', dtype=core_dtypes_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "DX_series = pd.concat([data_df['DX'+str(j)] for j in range(2, 31)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "DX_freq = DX_series.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13131679043651512"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(DX_freq>200)/len(DX_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9543447444167605"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(DX_series.loc[DX_series.isin(DX_freq.loc[DX_freq>200].index)])/len(DX_series.dropna())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare hyper-parameters and generate the .sh files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For embedding+NN with all codes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!rm hypertune*.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = ['no_mask']\n",
    "code_embed_dims = [200]\n",
    "fc_widths = [512]\n",
    "md_widths = [128]\n",
    "lr1s = [2e-4]\n",
    "lr2s = [2e-5]\n",
    "dropouts = [0, 0.3]\n",
    "batchsizes = [256]\n",
    "penalties = [0.]\n",
    "count_caps = [20]\n",
    "tst_seeds = range(10)\n",
    "cohorts = ['ami']\n",
    "zips = [(embed_dim, tst_fold, cohort, \n",
    "         'all/sepdx1/test_embed/cosine/embed_mat_{0}_{1:.3f}_{2}_{3}{4}.npy'.format(embed_dim, penalty, count_cap, cohort, \n",
    "                                                                                    tst_fold)) \n",
    "        for embed_dim in code_embed_dims for penalty in penalties for count_cap in count_caps for cohort in cohorts \n",
    "        for tst_fold in tst_seeds] \n",
    "#zips = zips + [(embed_dim, tst_fold, '') for embed_dim in code_embed_dims for tst_fold in tst_seeds]\n",
    "sep_dx1s = [1]\n",
    "val_folds = [7]\n",
    "rho_widths = [16]\n",
    "result_files = ['output/ht_result1121_{}.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = ['permutate_hosp']\n",
    "code_embed_dims = [200]\n",
    "fc_widths = [512]\n",
    "md_widths = [128]\n",
    "lr1s = [2e-4]\n",
    "lr2s = [2e-5]\n",
    "dropouts = [0]\n",
    "batchsizes = [256]\n",
    "penalties = [0.]\n",
    "count_caps = [20]\n",
    "tst_seeds = range(10)\n",
    "cohorts = ['ami']\n",
    "zips = [(embed_dim, tst_fold, cohort, \n",
    "         'all/sepdx1/test_embed/cosine/embed_mat_{0}_{1:.3f}_{2}_{3}{4}.npy'.format(embed_dim, penalty, count_cap, cohort, \n",
    "                                                                                    tst_fold)) \n",
    "        for embed_dim in code_embed_dims for penalty in penalties for count_cap in count_caps for cohort in cohorts \n",
    "        for tst_fold in tst_seeds] \n",
    "#zips = zips + [(embed_dim, tst_fold, '') for embed_dim in code_embed_dims for tst_fold in tst_seeds]\n",
    "sep_dx1s = [1]\n",
    "val_folds = [7]\n",
    "rho_widths = [16]\n",
    "result_files = ['output/ht_result1121_{}.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = ['demo', 'demo_dx1', 'demo_dx1_dx', 'demo_dx1_dx_pr']\n",
    "code_embed_dims = [200]\n",
    "fc_widths = [512]\n",
    "md_widths = [128]\n",
    "lr1s = [2e-4]\n",
    "lr2s = [2e-5]\n",
    "dropouts = [0.3]\n",
    "batchsizes = [256]\n",
    "penalties = [0.]\n",
    "count_caps = [20]\n",
    "tst_seeds = range(10)\n",
    "cohorts = ['ami']\n",
    "zips = [(embed_dim, tst_fold, cohort, \n",
    "         'all/sepdx1/test_embed/cosine/embed_mat_{0}_{1:.3f}_{2}_{3}{4}.npy'.format(embed_dim, penalty, count_cap, cohort, \n",
    "                                                                                    tst_fold)) \n",
    "        for embed_dim in code_embed_dims for penalty in penalties for count_cap in count_caps for cohort in cohorts \n",
    "        for tst_fold in tst_seeds] \n",
    "#zips = zips + [(embed_dim, tst_fold, '') for embed_dim in code_embed_dims for tst_fold in tst_seeds]\n",
    "sep_dx1s = [1]\n",
    "val_folds = [7]\n",
    "rho_widths = [16]\n",
    "result_files = ['output/ht_result1123_{}.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = ['dense_rho']\n",
    "code_embed_dims = [200]\n",
    "fc_widths = [512]\n",
    "md_widths = [128]\n",
    "lr1s = [2e-4]\n",
    "lr2s = [2e-5]\n",
    "dropouts = [0, 0.3]\n",
    "batchsizes = [256]\n",
    "penalties = [0., 0.3]\n",
    "count_caps = [20]\n",
    "tst_seeds = range(10)\n",
    "cohorts = ['ami']\n",
    "zips = [(embed_dim, tst_fold, cohort, \n",
    "         'all/sepdx1/test_embed/cosine/embed_mat_{0}_{1:.3f}_{2}_{3}{4}.npy'.format(embed_dim, penalty, count_cap, cohort, \n",
    "                                                                                    tst_fold)) \n",
    "        for embed_dim in code_embed_dims for penalty in penalties for count_cap in count_caps for cohort in cohorts \n",
    "        for tst_fold in tst_seeds] \n",
    "#zips = zips + [(embed_dim, tst_fold, '') for embed_dim in code_embed_dims for tst_fold in tst_seeds]\n",
    "sep_dx1s = [1]\n",
    "val_folds = [7]\n",
    "rho_widths = [16, 32, 64]\n",
    "result_files = ['output/ht_result1121_{}.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "para_itr = itertools.product(model_names, fc_widths, md_widths, lr1s, lr2s, dropouts, batchsizes, zips, sep_dx1s, \n",
    "                             val_folds, rho_widths, result_files)\n",
    "\n",
    "para_lst = [(mo, z[0], fc, md, lr1, lr2, dr, ba, z[3], z[1], z[2], se, va, rw, re) for mo, fc, md, lr1, lr2, dr, ba, z, se, va, rw, re in para_itr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(para_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_jobs = 5\n",
    "for para, job_ind in zip(para_lst, itertools.cycle(range(n_jobs))):\n",
    "    with open('hypertune'+str(job_ind)+'.sh', 'a') as f:\n",
    "        f.write('python train_template_all0827.py --model_name {} --code_embed_dim {} --fc_width {} --md_width {} --lr1 {} --lr2 {} --dropout {} --batchsize {} --embed_file {} --tst_seed {} --cohort {} --sep_dx1 {} --val_fold {} --rho_width {} --result_file {} --job_index {}\\n'.format(*para, job_ind))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_index = 5\n",
    "for para in para_lst:\n",
    "    with open('hypertune'+str(job_index)+'.sh', 'a') as f:\n",
    "        f.write('python train_template_all0827.py --model_name {0} --code_embed_dim {1} --fc_width {2} --md_width {3} --lr1 {4} --lr2 {5} --dropout {6} --batchsize {7} --embed_file {8} --tst_seed {9} --cohort {10} --sep_dx1 {11} --val_fold {12} --result_file {13} --job_index {14}\\n'.format(*para, job_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in [0, 1, 2, 5]:\n",
    "    for cc in [20, 100, 500]:\n",
    "        for f in range(10):\n",
    "            os.rename(path+'all/sepdx1/embed_mat_200_{:.3f}_{}_{}.npy'.format(p, cc, f), path+'all/sepdx1/embed_mat_200_{:.3f}_{}_ami{}.npy'.format(p, cc, f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention with all codes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!rm hypertune*.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = ['att_sum_nn']\n",
    "code_embed_dims = [256]\n",
    "fc_widths = [512]\n",
    "#md_widths = [128]\n",
    "lr1s = [2e-4]\n",
    "lr2s = [5e-5]\n",
    "dropouts = [0.3]\n",
    "batchsizes = [512]\n",
    "embed_files = ['pretrain']\n",
    "cohorts = ['ami', 'chf', 'pna']\n",
    "tst_seeds = range(10)\n",
    "val_folds = [9]\n",
    "penalties = [0.]\n",
    "penalty_metrics = ['l2']\n",
    "count_caps = [5]\n",
    "DX1_rarecutpoints = [10]\n",
    "DX_rarecutpoints = [10]\n",
    "PR_rarecutpoints = [10]\n",
    "other_preds = [0]\n",
    "ndxprs = [0]\n",
    "n_headss = [32]\n",
    "att_use_biass = [0]\n",
    "att_activations = ['linear']\n",
    "n_att_layerss = [4]\n",
    "result_files = ['output/ht_result0212_{}.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "para_itr = itertools.product(model_names, code_embed_dims, fc_widths, lr1s, lr2s, dropouts, batchsizes, embed_files, cohorts, \n",
    "                             tst_seeds, val_folds, penalties, penalty_metrics, count_caps, DX1_rarecutpoints, DX_rarecutpoints, \n",
    "                             PR_rarecutpoints, other_preds, ndxprs, n_headss, att_use_biass, att_activations, n_att_layerss, \n",
    "                             result_files)\n",
    "\n",
    "para_lst = list(para_itr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(para_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_jobs = 11\n",
    "for para, job_ind in zip(para_lst, itertools.cycle(range(n_jobs))):\n",
    "    with open('hypertune'+str(job_ind)+'.sh', 'a') as f:\n",
    "        f.write('python template_all_attention0201.py --model_name {} --code_embed_dim {} --fc_width {} --lr1 {} --lr2 {} --dropout {} --batchsize {} --embed_file {} --cohort {} --tst_seed {} --val_fold {} --penalty {} --penalty_metric {} --count_cap {} --dx1_rarecutpoint {} --dx_rarecutpoint {} --pr_rarecutpoint {} --other_pred {} --ndxpr {} --n_heads {} --att_use_bias {} --att_activation {} --n_att_layers {} --result_file {} --job_index {}\\n'.format(*para, job_ind))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For embedding+NN with a subset of codes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!rm hypertune*.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = ['setsum_nn']\n",
    "code_embed_dims = [200]\n",
    "fc_widths = [512]\n",
    "md_widths = [128]\n",
    "lr1s = [2e-4]\n",
    "lr2s = [2e-5]\n",
    "dropouts = [0.3]\n",
    "batchsizes = [512]\n",
    "embed_mats = ['pretrain']\n",
    "penalties = [0]\n",
    "penalty_metrics = ['cosine']\n",
    "count_caps = [5]\n",
    "tst_seeds = [0]\n",
    "cohorts = ['ami']\n",
    "DX_rarecutpoints = [10]\n",
    "PR_rarecutpoints = [drp/2 for drp in DX_rarecutpoints]\n",
    "val_folds = [5]\n",
    "result_files = ['output/ht_result1212_{}.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = ['embed_sum', 'embed_pool']\n",
    "code_embed_dims = [100]\n",
    "fc_widths = [512]\n",
    "md_widths = [128]\n",
    "lr1s = [2e-4]\n",
    "lr2s = [2e-5]\n",
    "dropouts = [0.3]\n",
    "batchsizes = [256]\n",
    "embed_mats = ['random']\n",
    "penalties = [0]\n",
    "penalty_metrics = ['cosine']\n",
    "count_caps = [5]\n",
    "tst_seeds = range(10)\n",
    "cohorts = ['ami']\n",
    "DX_rarecutpoints = [20]\n",
    "PR_rarecutpoints = [drp/2 for drp in DX_rarecutpoints]\n",
    "val_folds = [5]\n",
    "result_files = ['output/ht_result1001_{}.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "para_itr = itertools.product(model_names, code_embed_dims, fc_widths, md_widths, lr1s, lr2s, dropouts, batchsizes, embed_mats, \n",
    "                             penalties, penalty_metrics, count_caps, tst_seeds, cohorts, DX_rarecutpoints,\n",
    "                             val_folds, result_files)\n",
    "para_lst = [(mn, ced, fc, md, l1, l2, do, bs, em, p, pm, cc, ts, ch, dx, int(dx/2), vf, rf) \n",
    "            for mn, ced, fc, md, l1, l2, do, bs, em, p, pm, cc, ts, ch, dx, vf, rf in para_itr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(para_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_jobs = 4\n",
    "for para, job_ind in zip(para_lst, itertools.cycle(range(n_jobs))):\n",
    "    with open('hypertune'+str(job_ind)+'.sh', 'a') as f:\n",
    "        f.write('python train_template_sub0922.py --model_name {0} --code_embed_dim {1} --fc_width {2} --md_width {3} --lr1 {4} --lr2 {5} --dropout {6} --batchsize {7} --embed_file {8} --penalty {9} --penalty_metric {10} --count_cap {11} --tst_seed {12} --cohort {13} --dx_rarecutpoint {14} --pr_rarecutpoint {15} --val_fold {16} --result_file {17} --job_index {18}\\n'.format(*para, job_ind))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_index = 10\n",
    "for para in para_lst:\n",
    "    with open('hypertune'+str(job_index)+'.sh', 'a') as f:\n",
    "        f.write('python train_template_sub0922.py --model_name {0} --code_embed_dim {1} --fc_width {2} --md_width {3} --lr1 {4} --lr2 {5} --dropout {6} --batchsize {7} --embed_file {8} --penalty {9} --penalty_metric {10} --count_cap {11} --tst_seed {12} --cohort {13} --dx_rarecutpoint {14} --pr_rarecutpoint {15} --val_fold {16} --result_file {17} --job_index {18}\\n'.format(*para, job_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sample = 33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = np.random.choice(['setsum_nn'], n_sample)\n",
    "code_embed_dims = np.random.choice([200, 300], n_sample)\n",
    "fc_widths = np.random.choice([512, 1024], n_sample)\n",
    "md_widths = np.random.choice([128, 256], n_sample)\n",
    "lr1s = np.random.choice([2e-4], n_sample)\n",
    "lr2s = np.random.choice([2e-5], n_sample)\n",
    "dropouts = np.random.choice([0.3], n_sample)\n",
    "batchsizes = np.random.choice([256, 512], n_sample)\n",
    "embed_mats = np.random.choice(['pretrain'], n_sample)\n",
    "penalties = np.random.choice([0, 0.5, 1.], n_sample)\n",
    "penalty_metrics = np.random.choice(['cosine'], n_sample)\n",
    "count_caps = np.random.choice([0, 5, 20], n_sample)\n",
    "cohorts = np.random.choice(['ami'], n_sample)\n",
    "DX_rarecutpoints = np.random.choice([20], n_sample)\n",
    "PR_rarecutpoints = [int(drp/2) for drp in DX_rarecutpoints]\n",
    "val_folds = np.random.choice([5], n_sample)\n",
    "result_files = ['output/ht_result1001_{}.csv']*n_sample\n",
    "\n",
    "zips = zip(model_names, code_embed_dims, fc_widths, md_widths, lr1s, lr2s, dropouts, batchsizes, embed_mats, \n",
    "                             penalties, penalty_metrics, count_caps, cohorts, DX_rarecutpoints, PR_rarecutpoints,\n",
    "                             val_folds, result_files)\n",
    "tst_seeds = range(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "para_itr = itertools.product(zips, tst_seeds)\n",
    "\n",
    "para_lst = [(*z, t) for z, t in para_itr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "330"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(para_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_jobs = 4\n",
    "for para, job_ind in zip(para_lst, itertools.cycle(range(n_jobs))):\n",
    "    with open('hypertune'+str(job_ind)+'.sh', 'a') as f:\n",
    "        f.write('python train_template_sub0922.py --model_name {0} --code_embed_dim {1} --fc_width {2} --md_width {3} --lr1 {4} --lr2 {5} --dropout {6} --batchsize {7} --embed_file {8} --penalty {9} --penalty_metric {10} --count_cap {11} --cohort {12} --dx_rarecutpoint {13} --pr_rarecutpoint {14} --val_fold {15} --result_file {16} --tst_seed {17} --job_index {18}\\n'.format(*para, job_ind))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For OHE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!rm hypertune*.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_width1s = [1024]\n",
    "fc_width2s = [256]\n",
    "lrs = [1e-4]\n",
    "dropouts = [0.3]\n",
    "batchsizes = [512]\n",
    "tst_seeds = range(10)\n",
    "cohorts = ['ami', 'chf', 'pna']\n",
    "val_folds = [5]\n",
    "dx_rarecutpoints = [10]\n",
    "pr_rarecutpoints = [10]\n",
    "result_files = ['output/ht_result0223_{}.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "para_itr = itertools.product(fc_width1s, fc_width2s, lrs, dropouts, batchsizes, tst_seeds, cohorts, val_folds, \n",
    "                             dx_rarecutpoints, pr_rarecutpoints, result_files)\n",
    "\n",
    "para_lst = list(para_itr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(para_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_jobs = 1\n",
    "for para, job_ind in zip(para_lst, itertools.cycle(range(n_jobs))):\n",
    "    with open('hypertune'+str(job_ind)+'.sh', 'a') as f:\n",
    "        f.write('python train_template_ohe0925.py --fc_width1 {} --fc_width2 {} --lr {} --dropout {} --batchsize {} --tst_seed {} --cohort {} --val_fold {} --dx_rarecutpoint {} --pr_rarecutpoint {} --result_file {} --job_index {}\\n'.format(*para, job_ind))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_index = 0\n",
    "for para in para_lst:\n",
    "    with open('hypertune'+str(job_index)+'.sh', 'a') as f:\n",
    "        f.write('python train_template_all0827.py --model_name {0} --code_embed_dim {1} --fc_width {2} --md_width {3} --lr1 {4} --lr2 {5} --dropout {6} --batchsize {7} --embed_file {8} --tst_seed {9} --cohort {10} --sep_dx1 {11} --val_fold {12} --result_file {13} --job_index {14}\\n'.format(*para, job_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding + NN with all codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for job_ind in range(5):\n",
    "    df = pd.read_csv('output/ht_result1123_'+str(job_ind)+'.csv', \n",
    "                     names=['model_name', 'code_embed_dim', 'hosp_embed_dim', 'fc_width', 'md_width', 'lr1', 'lr2', 'dropout',\n",
    "                            'batchsize', 'embed_file', 'cohort', 'sep_dx1', 'tst_seed', 'n_fold', 'auc_mean', 'auc_avg', \n",
    "                            'auc_freeze', 'rho_width', 'y_pred_file'], index_col=None)\n",
    "    res = pd.concat([res, df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = res.assign(cohort_seed=res.cohort+res.tst_seed.apply(lambda x:str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = res.drop_duplicates(subset=['cohort_seed'], keep='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>code_embed_dim</th>\n",
       "      <th>hosp_embed_dim</th>\n",
       "      <th>fc_width</th>\n",
       "      <th>md_width</th>\n",
       "      <th>lr1</th>\n",
       "      <th>lr2</th>\n",
       "      <th>dropout</th>\n",
       "      <th>batchsize</th>\n",
       "      <th>embed_file</th>\n",
       "      <th>cohort</th>\n",
       "      <th>sep_dx1</th>\n",
       "      <th>tst_seed</th>\n",
       "      <th>n_fold</th>\n",
       "      <th>auc_mean</th>\n",
       "      <th>auc_avg</th>\n",
       "      <th>auc_freeze</th>\n",
       "      <th>rho_width</th>\n",
       "      <th>y_pred_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>demo</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.3</td>\n",
       "      <td>256</td>\n",
       "      <td>all/sepdx1/test_embed/cosine/embed_mat_200_0.0...</td>\n",
       "      <td>ami</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.6001</td>\n",
       "      <td>0.6000</td>\n",
       "      <td>0.6000</td>\n",
       "      <td>16</td>\n",
       "      <td>output/y_pred_mat18_11_24_03_16_32.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>demo</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.3</td>\n",
       "      <td>256</td>\n",
       "      <td>all/sepdx1/test_embed/cosine/embed_mat_200_0.0...</td>\n",
       "      <td>ami</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0.5983</td>\n",
       "      <td>0.5983</td>\n",
       "      <td>0.5983</td>\n",
       "      <td>16</td>\n",
       "      <td>output/y_pred_mat18_11_24_03_37_27.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>demo_dx1</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.3</td>\n",
       "      <td>256</td>\n",
       "      <td>all/sepdx1/test_embed/cosine/embed_mat_200_0.0...</td>\n",
       "      <td>ami</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.6080</td>\n",
       "      <td>0.6081</td>\n",
       "      <td>0.6080</td>\n",
       "      <td>16</td>\n",
       "      <td>output/y_pred_mat18_11_24_04_10_36.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>demo_dx1</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.3</td>\n",
       "      <td>256</td>\n",
       "      <td>all/sepdx1/test_embed/cosine/embed_mat_200_0.0...</td>\n",
       "      <td>ami</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0.6056</td>\n",
       "      <td>0.6057</td>\n",
       "      <td>0.6058</td>\n",
       "      <td>16</td>\n",
       "      <td>output/y_pred_mat18_11_24_04_43_26.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>demo_dx1_dx</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.3</td>\n",
       "      <td>256</td>\n",
       "      <td>all/sepdx1/test_embed/cosine/embed_mat_200_0.0...</td>\n",
       "      <td>ami</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.7107</td>\n",
       "      <td>0.7123</td>\n",
       "      <td>0.7099</td>\n",
       "      <td>16</td>\n",
       "      <td>output/y_pred_mat18_11_24_05_26_27.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>demo_dx1_dx</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.3</td>\n",
       "      <td>256</td>\n",
       "      <td>all/sepdx1/test_embed/cosine/embed_mat_200_0.0...</td>\n",
       "      <td>ami</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0.7032</td>\n",
       "      <td>0.7050</td>\n",
       "      <td>0.7030</td>\n",
       "      <td>16</td>\n",
       "      <td>output/y_pred_mat18_11_24_06_10_05.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>demo_dx1_dx_pr</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.3</td>\n",
       "      <td>256</td>\n",
       "      <td>all/sepdx1/test_embed/cosine/embed_mat_200_0.0...</td>\n",
       "      <td>ami</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.7148</td>\n",
       "      <td>0.7171</td>\n",
       "      <td>0.7131</td>\n",
       "      <td>16</td>\n",
       "      <td>output/y_pred_mat18_11_24_07_05_57.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>demo_dx1_dx_pr</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.3</td>\n",
       "      <td>256</td>\n",
       "      <td>all/sepdx1/test_embed/cosine/embed_mat_200_0.0...</td>\n",
       "      <td>ami</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0.7073</td>\n",
       "      <td>0.7092</td>\n",
       "      <td>0.7061</td>\n",
       "      <td>16</td>\n",
       "      <td>output/y_pred_mat18_11_24_08_03_00.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>demo</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.3</td>\n",
       "      <td>256</td>\n",
       "      <td>all/sepdx1/test_embed/cosine/embed_mat_200_0.0...</td>\n",
       "      <td>ami</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0.6095</td>\n",
       "      <td>0.6094</td>\n",
       "      <td>0.6096</td>\n",
       "      <td>16</td>\n",
       "      <td>output/y_pred_mat18_11_24_03_15_37.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>demo</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.3</td>\n",
       "      <td>256</td>\n",
       "      <td>all/sepdx1/test_embed/cosine/embed_mat_200_0.0...</td>\n",
       "      <td>ami</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>0.5888</td>\n",
       "      <td>0.5886</td>\n",
       "      <td>0.5887</td>\n",
       "      <td>16</td>\n",
       "      <td>output/y_pred_mat18_11_24_03_34_25.npy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       model_name  code_embed_dim  hosp_embed_dim  fc_width  md_width     lr1  \\\n",
       "0            demo             200               1       512       128  0.0002   \n",
       "1            demo             200               1       512       128  0.0002   \n",
       "2        demo_dx1             200               1       512       128  0.0002   \n",
       "3        demo_dx1             200               1       512       128  0.0002   \n",
       "4     demo_dx1_dx             200               1       512       128  0.0002   \n",
       "5     demo_dx1_dx             200               1       512       128  0.0002   \n",
       "6  demo_dx1_dx_pr             200               1       512       128  0.0002   \n",
       "7  demo_dx1_dx_pr             200               1       512       128  0.0002   \n",
       "0            demo             200               1       512       128  0.0002   \n",
       "1            demo             200               1       512       128  0.0002   \n",
       "\n",
       "       lr2  dropout  batchsize  \\\n",
       "0  0.00002      0.3        256   \n",
       "1  0.00002      0.3        256   \n",
       "2  0.00002      0.3        256   \n",
       "3  0.00002      0.3        256   \n",
       "4  0.00002      0.3        256   \n",
       "5  0.00002      0.3        256   \n",
       "6  0.00002      0.3        256   \n",
       "7  0.00002      0.3        256   \n",
       "0  0.00002      0.3        256   \n",
       "1  0.00002      0.3        256   \n",
       "\n",
       "                                          embed_file cohort  sep_dx1  \\\n",
       "0  all/sepdx1/test_embed/cosine/embed_mat_200_0.0...    ami        1   \n",
       "1  all/sepdx1/test_embed/cosine/embed_mat_200_0.0...    ami        1   \n",
       "2  all/sepdx1/test_embed/cosine/embed_mat_200_0.0...    ami        1   \n",
       "3  all/sepdx1/test_embed/cosine/embed_mat_200_0.0...    ami        1   \n",
       "4  all/sepdx1/test_embed/cosine/embed_mat_200_0.0...    ami        1   \n",
       "5  all/sepdx1/test_embed/cosine/embed_mat_200_0.0...    ami        1   \n",
       "6  all/sepdx1/test_embed/cosine/embed_mat_200_0.0...    ami        1   \n",
       "7  all/sepdx1/test_embed/cosine/embed_mat_200_0.0...    ami        1   \n",
       "0  all/sepdx1/test_embed/cosine/embed_mat_200_0.0...    ami        1   \n",
       "1  all/sepdx1/test_embed/cosine/embed_mat_200_0.0...    ami        1   \n",
       "\n",
       "   tst_seed  n_fold  auc_mean  auc_avg  auc_freeze  rho_width  \\\n",
       "0         0       7    0.6001   0.6000      0.6000         16   \n",
       "1         5       7    0.5983   0.5983      0.5983         16   \n",
       "2         0       7    0.6080   0.6081      0.6080         16   \n",
       "3         5       7    0.6056   0.6057      0.6058         16   \n",
       "4         0       7    0.7107   0.7123      0.7099         16   \n",
       "5         5       7    0.7032   0.7050      0.7030         16   \n",
       "6         0       7    0.7148   0.7171      0.7131         16   \n",
       "7         5       7    0.7073   0.7092      0.7061         16   \n",
       "0         1       7    0.6095   0.6094      0.6096         16   \n",
       "1         6       7    0.5888   0.5886      0.5887         16   \n",
       "\n",
       "                              y_pred_file  \n",
       "0  output/y_pred_mat18_11_24_03_16_32.npy  \n",
       "1  output/y_pred_mat18_11_24_03_37_27.npy  \n",
       "2  output/y_pred_mat18_11_24_04_10_36.npy  \n",
       "3  output/y_pred_mat18_11_24_04_43_26.npy  \n",
       "4  output/y_pred_mat18_11_24_05_26_27.npy  \n",
       "5  output/y_pred_mat18_11_24_06_10_05.npy  \n",
       "6  output/y_pred_mat18_11_24_07_05_57.npy  \n",
       "7  output/y_pred_mat18_11_24_08_03_00.npy  \n",
       "0  output/y_pred_mat18_11_24_03_15_37.npy  \n",
       "1  output/y_pred_mat18_11_24_03_34_25.npy  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    cosine\n",
       "1    cosine\n",
       "2    cosine\n",
       "3    cosine\n",
       "4    cosine\n",
       "Name: embed_file, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.embed_file.apply(lambda x:x.split('/')[3]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = res.assign(penalty = res.embed_file.apply(lambda x:float(x.split('_')[4])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = res.assign(count_cap = res.embed_file.apply(lambda x:float(x.split('_')[5])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = res.assign(metric = res.embed_file.apply(lambda x:(x.split('/')[3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = res.loc[res.penalty==0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_grouped = res.groupby(['model_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">auc_freeze</th>\n",
       "      <th colspan=\"2\" halign=\"left\">auc_mean</th>\n",
       "      <th colspan=\"2\" halign=\"left\">auc_avg</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>demo</th>\n",
       "      <td>0.60118</td>\n",
       "      <td>10</td>\n",
       "      <td>0.60131</td>\n",
       "      <td>10</td>\n",
       "      <td>0.60132</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>demo_dx1</th>\n",
       "      <td>0.60768</td>\n",
       "      <td>10</td>\n",
       "      <td>0.60796</td>\n",
       "      <td>10</td>\n",
       "      <td>0.60813</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>demo_dx1_dx</th>\n",
       "      <td>0.71121</td>\n",
       "      <td>10</td>\n",
       "      <td>0.71162</td>\n",
       "      <td>10</td>\n",
       "      <td>0.71352</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>demo_dx1_dx_pr</th>\n",
       "      <td>0.71357</td>\n",
       "      <td>10</td>\n",
       "      <td>0.71444</td>\n",
       "      <td>10</td>\n",
       "      <td>0.71653</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               auc_freeze       auc_mean        auc_avg      \n",
       "                     mean count     mean count     mean count\n",
       "model_name                                                   \n",
       "demo              0.60118    10  0.60131    10  0.60132    10\n",
       "demo_dx1          0.60768    10  0.60796    10  0.60813    10\n",
       "demo_dx1_dx       0.71121    10  0.71162    10  0.71352    10\n",
       "demo_dx1_dx_pr    0.71357    10  0.71444    10  0.71653    10"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_grouped[['auc_freeze', 'auc_mean', 'auc_avg']].agg(['mean', 'count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>code_embed_dim</th>\n",
       "      <th>fc_width</th>\n",
       "      <th>md_width</th>\n",
       "      <th>lr1</th>\n",
       "      <th>lr2</th>\n",
       "      <th>dropout</th>\n",
       "      <th>batchsize</th>\n",
       "      <th>cohort</th>\n",
       "      <th>tst_seed</th>\n",
       "      <th>n_fold</th>\n",
       "      <th>auc_mean</th>\n",
       "      <th>auc_avg</th>\n",
       "      <th>auc_freeze</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>demo_dx1_dx_pr</td>\n",
       "      <td>200</td>\n",
       "      <td>512</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.3</td>\n",
       "      <td>256</td>\n",
       "      <td>ami</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.7148</td>\n",
       "      <td>0.7171</td>\n",
       "      <td>0.7131</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       model_name  code_embed_dim  fc_width  md_width     lr1      lr2  \\\n",
       "6  demo_dx1_dx_pr             200       512       128  0.0002  0.00002   \n",
       "\n",
       "   dropout  batchsize cohort  tst_seed  n_fold  auc_mean  auc_avg  auc_freeze  \n",
       "6      0.3        256    ami         0       7    0.7148   0.7171      0.7131  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.loc[(res.model_name=='demo_dx1_dx_pr')&(res.tst_seed==0)].drop(['embed_file', 'rho_width', 'y_pred_file', 'sep_dx1', 'hosp_embed_dim'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, g in res_grouped:\n",
    "    if g.auc_avg.mean() == res_grouped.auc_avg.mean().max():\n",
    "        best = g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ami freeze: 0.7193 (0.0017) mean: 0.7202 (0.0017) avg: 0.7222 (0.0017)\n",
      "chf freeze: 0.6298 (0.0015) mean: 0.6309 (0.0015) avg: 0.6337 (0.0015)\n",
      "pna freeze: 0.6748 (0.0011) mean: 0.6758 (0.0011) avg: 0.6782 (0.0011)\n"
     ]
    }
   ],
   "source": [
    "for n, g in res_grouped:\n",
    "    print(n, 'freeze: {0:.4f} ({1:.4f})'.format(g.auc_freeze.mean(), g.auc_freeze.std()/np.sqrt(len(g))), \n",
    "         'mean: {0:.4f} ({1:.4f})'.format(g.auc_mean.mean(), g.auc_mean.std()/np.sqrt(len(g))), \n",
    "         'avg: {0:.4f} ({1:.4f})'.format(g.auc_avg.mean(), g.auc_avg.std()/np.sqrt(len(g))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "res2 = pd.read_csv('output/ht_result1001embed_lr_sub.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "res2 = res2.loc[(res2.penalty==0) & (res2.code_embed_dim==100) & (res2.fc_width==512) & (res2.md_width==128) & \n",
    "                  (res2.tst_seed==0) & (res2.count_cap==20)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>code_embed_dim</th>\n",
       "      <th>hosp_embed_dim</th>\n",
       "      <th>fc_width</th>\n",
       "      <th>md_width</th>\n",
       "      <th>lr1</th>\n",
       "      <th>lr2</th>\n",
       "      <th>dropout</th>\n",
       "      <th>batchsize</th>\n",
       "      <th>embed_file</th>\n",
       "      <th>...</th>\n",
       "      <th>n_fold</th>\n",
       "      <th>penalty</th>\n",
       "      <th>penalty_metric</th>\n",
       "      <th>count_cap</th>\n",
       "      <th>DX_rarecutpoint</th>\n",
       "      <th>PR_rarecutpoint</th>\n",
       "      <th>auc_mean</th>\n",
       "      <th>auc_avg</th>\n",
       "      <th>auc_freeze</th>\n",
       "      <th>y_pred_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>embed_sum</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.3</td>\n",
       "      <td>256</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>0.70943</td>\n",
       "      <td>0.71143</td>\n",
       "      <td>0.70883</td>\n",
       "      <td>output/y_pred_mat18_10_01_01_39_44.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>embed_pool</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.3</td>\n",
       "      <td>256</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>0.70872</td>\n",
       "      <td>0.71081</td>\n",
       "      <td>0.70811</td>\n",
       "      <td>output/y_pred_mat18_10_02_04_22_47.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>embed_sum</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.3</td>\n",
       "      <td>256</td>\n",
       "      <td>random</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>0.70836</td>\n",
       "      <td>0.71009</td>\n",
       "      <td>0.70551</td>\n",
       "      <td>output/y_pred_mat18_10_02_11_28_17.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>embed_pool</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.3</td>\n",
       "      <td>256</td>\n",
       "      <td>random</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>0.70814</td>\n",
       "      <td>0.71010</td>\n",
       "      <td>0.70570</td>\n",
       "      <td>output/y_pred_mat18_10_03_12_08_28.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>embed_pool</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.3</td>\n",
       "      <td>256</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>0.70958</td>\n",
       "      <td>0.71159</td>\n",
       "      <td>0.70900</td>\n",
       "      <td>output/y_pred_mat18_10_02_07_09_48.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>embed_sum</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.3</td>\n",
       "      <td>256</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>0.71029</td>\n",
       "      <td>0.71210</td>\n",
       "      <td>0.70942</td>\n",
       "      <td>output/y_pred_mat18_10_01_02_08_18.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>embed_sum</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.3</td>\n",
       "      <td>256</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>100</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>0.70923</td>\n",
       "      <td>0.71134</td>\n",
       "      <td>0.70865</td>\n",
       "      <td>output/y_pred_mat18_10_01_02_51_44.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>embed_pool</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.3</td>\n",
       "      <td>256</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>100</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>0.70993</td>\n",
       "      <td>0.71209</td>\n",
       "      <td>0.70884</td>\n",
       "      <td>output/y_pred_mat18_10_02_08_10_52.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>embed_sum</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.3</td>\n",
       "      <td>256</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>300</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>0.71028</td>\n",
       "      <td>0.71208</td>\n",
       "      <td>0.70937</td>\n",
       "      <td>output/y_pred_mat18_10_01_04_10_20.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>embed_pool</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.3</td>\n",
       "      <td>256</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>300</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>0.70935</td>\n",
       "      <td>0.71113</td>\n",
       "      <td>0.70839</td>\n",
       "      <td>output/y_pred_mat18_10_02_06_36_01.npy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     model_name  code_embed_dim  hosp_embed_dim  fc_width  md_width     lr1  \\\n",
       "0     embed_sum             100               1       512       128  0.0002   \n",
       "175  embed_pool             100               1       512       128  0.0002   \n",
       "67    embed_sum             100               1       512       128  0.0002   \n",
       "349  embed_pool             100               1       512       128  0.0002   \n",
       "35   embed_pool             100               1       512       128  0.0002   \n",
       "283   embed_sum             100               1       512       128  0.0002   \n",
       "145   embed_sum             100               1       512       128  0.0002   \n",
       "318  embed_pool             100               1       512       128  0.0002   \n",
       "5     embed_sum             100               1       512       128  0.0002   \n",
       "180  embed_pool             100               1       512       128  0.0002   \n",
       "\n",
       "         lr2  dropout  batchsize embed_file  \\\n",
       "0    0.00002      0.3        256   pretrain   \n",
       "175  0.00002      0.3        256   pretrain   \n",
       "67   0.00002      0.3        256     random   \n",
       "349  0.00002      0.3        256     random   \n",
       "35   0.00002      0.3        256   pretrain   \n",
       "283  0.00002      0.3        256   pretrain   \n",
       "145  0.00002      0.3        256   pretrain   \n",
       "318  0.00002      0.3        256   pretrain   \n",
       "5    0.00002      0.3        256   pretrain   \n",
       "180  0.00002      0.3        256   pretrain   \n",
       "\n",
       "                      ...                   n_fold  penalty  penalty_metric  \\\n",
       "0                     ...                        5      0.0          cosine   \n",
       "175                   ...                        5      0.0          cosine   \n",
       "67                    ...                        5      0.0          cosine   \n",
       "349                   ...                        5      0.0          cosine   \n",
       "35                    ...                        5      0.0          cosine   \n",
       "283                   ...                        5      0.0          cosine   \n",
       "145                   ...                        5      0.0          cosine   \n",
       "318                   ...                        5      0.0          cosine   \n",
       "5                     ...                        5      0.0          cosine   \n",
       "180                   ...                        5      0.0          cosine   \n",
       "\n",
       "     count_cap DX_rarecutpoint  PR_rarecutpoint  auc_mean  auc_avg  \\\n",
       "0            1              20               10   0.70943  0.71143   \n",
       "175          1              20               10   0.70872  0.71081   \n",
       "67           5              20               10   0.70836  0.71009   \n",
       "349          5              20               10   0.70814  0.71010   \n",
       "35          20              20               10   0.70958  0.71159   \n",
       "283         20              20               10   0.71029  0.71210   \n",
       "145        100              20               10   0.70923  0.71134   \n",
       "318        100              20               10   0.70993  0.71209   \n",
       "5          300              20               10   0.71028  0.71208   \n",
       "180        300              20               10   0.70935  0.71113   \n",
       "\n",
       "     auc_freeze                             y_pred_file  \n",
       "0       0.70883  output/y_pred_mat18_10_01_01_39_44.npy  \n",
       "175     0.70811  output/y_pred_mat18_10_02_04_22_47.npy  \n",
       "67      0.70551  output/y_pred_mat18_10_02_11_28_17.npy  \n",
       "349     0.70570  output/y_pred_mat18_10_03_12_08_28.npy  \n",
       "35      0.70900  output/y_pred_mat18_10_02_07_09_48.npy  \n",
       "283     0.70942  output/y_pred_mat18_10_01_02_08_18.npy  \n",
       "145     0.70865  output/y_pred_mat18_10_01_02_51_44.npy  \n",
       "318     0.70884  output/y_pred_mat18_10_02_08_10_52.npy  \n",
       "5       0.70937  output/y_pred_mat18_10_01_04_10_20.npy  \n",
       "180     0.70839  output/y_pred_mat18_10_02_06_36_01.npy  \n",
       "\n",
       "[10 rows x 22 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res2.loc[(res2.tst_seed==0) & (res2.penalty==0)].sort_values('count_cap')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">auc_freeze</th>\n",
       "      <th colspan=\"2\" halign=\"left\">auc_mean</th>\n",
       "      <th colspan=\"2\" halign=\"left\">auc_avg</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>code_embed_dim</th>\n",
       "      <th>fc_width</th>\n",
       "      <th>md_width</th>\n",
       "      <th>batchsize</th>\n",
       "      <th>count_cap</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <th>512</th>\n",
       "      <th>128</th>\n",
       "      <th>256</th>\n",
       "      <th>20</th>\n",
       "      <td>0.70921</td>\n",
       "      <td>2</td>\n",
       "      <td>0.709935</td>\n",
       "      <td>2</td>\n",
       "      <td>0.711845</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     auc_freeze        \\\n",
       "                                                           mean count   \n",
       "code_embed_dim fc_width md_width batchsize count_cap                    \n",
       "100            512      128      256       20           0.70921     2   \n",
       "\n",
       "                                                      auc_mean        \\\n",
       "                                                          mean count   \n",
       "code_embed_dim fc_width md_width batchsize count_cap                   \n",
       "100            512      128      256       20         0.709935     2   \n",
       "\n",
       "                                                       auc_avg        \n",
       "                                                          mean count  \n",
       "code_embed_dim fc_width md_width batchsize count_cap                  \n",
       "100            512      128      256       20         0.711845     2  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res2.groupby(['code_embed_dim', 'fc_width', 'md_width', 'batchsize', 'count_cap'])[['auc_freeze', 'auc_mean', 'auc_avg']].agg(['mean', 'count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.to_csv('output/ht_result1123.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.read_csv('output/ht_result1001embed_lr_sub.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.concat([res, res2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x2b822428b978>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "res.plot.scatter('penalty', 'auc_avg', xlim=(0, 1), ylim=(0.725, 0.733))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = res.loc[res.md_width>100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x2ab55625ef28>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "res.plot.scatter('fc_width', 'auc_mean', ylim=(0.71, 0.735))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x2b5a2a76bf28>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "res.plot.scatter('batchsize', 'auc_mean', ylim=(0.72, 0.73))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>embed_file</th>\n",
       "      <th>y_pred_file</th>\n",
       "      <th>auc_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>all/embed_mat_pen6e-7.npy</td>\n",
       "      <td>output/y_pred_mat18_08_30_01_34_24.npy</td>\n",
       "      <td>0.7282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>all/embed_mat_pen9e-7.npy</td>\n",
       "      <td>output/y_pred_mat18_08_30_03_43_48.npy</td>\n",
       "      <td>0.7282</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  embed_file                             y_pred_file  auc_mean\n",
       "5  all/embed_mat_pen6e-7.npy  output/y_pred_mat18_08_30_01_34_24.npy    0.7282\n",
       "8  all/embed_mat_pen9e-7.npy  output/y_pred_mat18_08_30_03_43_48.npy    0.7282"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.loc[res.auc_mean==res.auc_mean.max(), ['embed_file', 'y_pred_file', 'auc_mean']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>code_embed_dim</th>\n",
       "      <th>hosp_embed_dim</th>\n",
       "      <th>fc_width</th>\n",
       "      <th>md_width</th>\n",
       "      <th>lr1</th>\n",
       "      <th>lr2</th>\n",
       "      <th>dropout</th>\n",
       "      <th>batchsize</th>\n",
       "      <th>embed_file</th>\n",
       "      <th>data_file</th>\n",
       "      <th>sep_dx1</th>\n",
       "      <th>tst_seed</th>\n",
       "      <th>n_fold</th>\n",
       "      <th>auc_mean</th>\n",
       "      <th>auc_cil</th>\n",
       "      <th>auc_cih</th>\n",
       "      <th>y_pred_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>setsum</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.1</td>\n",
       "      <td>128</td>\n",
       "      <td>all/embed_mat_pen_2_1e-6.npy</td>\n",
       "      <td>cohorts/ami/ami_pred.csv</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.7286</td>\n",
       "      <td>(0.7275</td>\n",
       "      <td>0.7298)</td>\n",
       "      <td>output/y_pred_mat18_08_30_11_48_48.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>setsum</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td>256</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>0.3</td>\n",
       "      <td>256</td>\n",
       "      <td>all/embed_mat_pen_2_1e-6.npy</td>\n",
       "      <td>cohorts/ami/ami_pred.csv</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.7286</td>\n",
       "      <td>(0.7278</td>\n",
       "      <td>0.7295)</td>\n",
       "      <td>output/y_pred_mat18_08_30_06_42_42.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>setsum</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>256</td>\n",
       "      <td>all/embed_mat_pen_2_1e-6.npy</td>\n",
       "      <td>cohorts/ami/ami_pred.csv</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.7287</td>\n",
       "      <td>(0.7275</td>\n",
       "      <td>0.7300)</td>\n",
       "      <td>output/y_pred_mat18_08_30_11_59_33.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>setsum</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td>256</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>0.1</td>\n",
       "      <td>256</td>\n",
       "      <td>all/embed_mat_pen_2_1e-6.npy</td>\n",
       "      <td>cohorts/ami/ami_pred.csv</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.7286</td>\n",
       "      <td>(0.7271</td>\n",
       "      <td>0.7301)</td>\n",
       "      <td>output/y_pred_mat18_08_30_08_38_31.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>setsum</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.5</td>\n",
       "      <td>256</td>\n",
       "      <td>all/embed_mat_pen_2_1e-6.npy</td>\n",
       "      <td>cohorts/ami/ami_pred.csv</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.7286</td>\n",
       "      <td>(0.7278</td>\n",
       "      <td>0.7294)</td>\n",
       "      <td>output/y_pred_mat18_08_30_12_04_01.npy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   model_name  code_embed_dim  hosp_embed_dim  fc_width  md_width     lr1  \\\n",
       "26     setsum             100               1       128       128  0.0002   \n",
       "35     setsum             100               1       128       256  0.0002   \n",
       "24     setsum             100               1       128       128  0.0002   \n",
       "34     setsum             100               1       128       256  0.0002   \n",
       "27     setsum             100               1       128       128  0.0002   \n",
       "\n",
       "        lr2  dropout  batchsize                    embed_file  \\\n",
       "26  0.00002      0.1        128  all/embed_mat_pen_2_1e-6.npy   \n",
       "35  0.00005      0.3        256  all/embed_mat_pen_2_1e-6.npy   \n",
       "24  0.00001      0.1        256  all/embed_mat_pen_2_1e-6.npy   \n",
       "34  0.00005      0.1        256  all/embed_mat_pen_2_1e-6.npy   \n",
       "27  0.00002      0.5        256  all/embed_mat_pen_2_1e-6.npy   \n",
       "\n",
       "                   data_file  sep_dx1  tst_seed  n_fold  auc_mean  auc_cil  \\\n",
       "26  cohorts/ami/ami_pred.csv        0         0       7    0.7286  (0.7275   \n",
       "35  cohorts/ami/ami_pred.csv        0         0       7    0.7286  (0.7278   \n",
       "24  cohorts/ami/ami_pred.csv        0         0       7    0.7287  (0.7275   \n",
       "34  cohorts/ami/ami_pred.csv        0         0       7    0.7286  (0.7271   \n",
       "27  cohorts/ami/ami_pred.csv        0         0       7    0.7286  (0.7278   \n",
       "\n",
       "     auc_cih                             y_pred_file  \n",
       "26   0.7298)  output/y_pred_mat18_08_30_11_48_48.npy  \n",
       "35   0.7295)  output/y_pred_mat18_08_30_06_42_42.npy  \n",
       "24   0.7300)  output/y_pred_mat18_08_30_11_59_33.npy  \n",
       "34   0.7301)  output/y_pred_mat18_08_30_08_38_31.npy  \n",
       "27   0.7294)  output/y_pred_mat18_08_30_12_04_01.npy  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.loc[res.auc_mean>0.7285, ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding + NN  with subset of codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for job_ind in range(4):\n",
    "    df = pd.read_csv('output/ht_result1001_'+str(job_ind)+'.csv', \n",
    "                     names=['model_name', 'code_embed_dim', 'hosp_embed_dim', 'fc_width', 'md_width', 'lr1', 'lr2', 'dropout',\n",
    "                            'batchsize', 'embed_file', 'cohort', 'tst_seed', 'n_fold', 'penalty', 'penalty_metric', 'count_cap', \n",
    "                            'DX_rarecutpoint', 'PR_rarecutpoint', 'auc_mean', 'auc_avg', 'auc_freeze', 'y_pred_file'], index_col=None)\n",
    "    res = pd.concat([res, df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = res.loc[res.model_name=='embed_sum']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>code_embed_dim</th>\n",
       "      <th>hosp_embed_dim</th>\n",
       "      <th>fc_width</th>\n",
       "      <th>md_width</th>\n",
       "      <th>lr1</th>\n",
       "      <th>lr2</th>\n",
       "      <th>dropout</th>\n",
       "      <th>batchsize</th>\n",
       "      <th>embed_file</th>\n",
       "      <th>...</th>\n",
       "      <th>n_fold</th>\n",
       "      <th>penalty</th>\n",
       "      <th>penalty_metric</th>\n",
       "      <th>count_cap</th>\n",
       "      <th>DX_rarecutpoint</th>\n",
       "      <th>PR_rarecutpoint</th>\n",
       "      <th>auc_mean</th>\n",
       "      <th>auc_avg</th>\n",
       "      <th>auc_freeze</th>\n",
       "      <th>y_pred_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>setsum_nn</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>1024</td>\n",
       "      <td>256</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.3</td>\n",
       "      <td>256</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>0.71854</td>\n",
       "      <td>0.72020</td>\n",
       "      <td>0.71711</td>\n",
       "      <td>output/y_pred_mat18_10_03_04_03_11.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>setsum_nn</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>1024</td>\n",
       "      <td>256</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.3</td>\n",
       "      <td>256</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>0.71977</td>\n",
       "      <td>0.72173</td>\n",
       "      <td>0.71875</td>\n",
       "      <td>output/y_pred_mat18_10_03_04_32_18.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>setsum_nn</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>1024</td>\n",
       "      <td>256</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.3</td>\n",
       "      <td>256</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>0.72522</td>\n",
       "      <td>0.72696</td>\n",
       "      <td>0.72325</td>\n",
       "      <td>output/y_pred_mat18_10_03_05_02_18.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>setsum_nn</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "      <td>256</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.3</td>\n",
       "      <td>256</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>0.71286</td>\n",
       "      <td>0.71549</td>\n",
       "      <td>0.71256</td>\n",
       "      <td>output/y_pred_mat18_10_03_05_34_03.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>setsum_nn</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "      <td>256</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.3</td>\n",
       "      <td>256</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>0.70873</td>\n",
       "      <td>0.71141</td>\n",
       "      <td>0.70968</td>\n",
       "      <td>output/y_pred_mat18_10_03_06_07_20.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>setsum_nn</td>\n",
       "      <td>300</td>\n",
       "      <td>1</td>\n",
       "      <td>1024</td>\n",
       "      <td>256</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.3</td>\n",
       "      <td>512</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>0.71711</td>\n",
       "      <td>0.71959</td>\n",
       "      <td>0.71640</td>\n",
       "      <td>output/y_pred_mat18_10_03_06_30_57.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>setsum_nn</td>\n",
       "      <td>300</td>\n",
       "      <td>1</td>\n",
       "      <td>1024</td>\n",
       "      <td>256</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.3</td>\n",
       "      <td>512</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>0.71793</td>\n",
       "      <td>0.72071</td>\n",
       "      <td>0.71688</td>\n",
       "      <td>output/y_pred_mat18_10_03_06_56_58.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>setsum_nn</td>\n",
       "      <td>300</td>\n",
       "      <td>1</td>\n",
       "      <td>1024</td>\n",
       "      <td>256</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.3</td>\n",
       "      <td>512</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>0.72293</td>\n",
       "      <td>0.72548</td>\n",
       "      <td>0.72196</td>\n",
       "      <td>output/y_pred_mat18_10_03_07_22_56.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>setsum_nn</td>\n",
       "      <td>300</td>\n",
       "      <td>1</td>\n",
       "      <td>1024</td>\n",
       "      <td>256</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.3</td>\n",
       "      <td>512</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>0.71236</td>\n",
       "      <td>0.71453</td>\n",
       "      <td>0.71221</td>\n",
       "      <td>output/y_pred_mat18_10_03_07_46_51.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>setsum_nn</td>\n",
       "      <td>300</td>\n",
       "      <td>1</td>\n",
       "      <td>1024</td>\n",
       "      <td>256</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.3</td>\n",
       "      <td>512</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>0.70821</td>\n",
       "      <td>0.71076</td>\n",
       "      <td>0.70844</td>\n",
       "      <td>output/y_pred_mat18_10_03_08_11_43.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>setsum_nn</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>1024</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.3</td>\n",
       "      <td>512</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>0.71723</td>\n",
       "      <td>0.72062</td>\n",
       "      <td>0.71692</td>\n",
       "      <td>output/y_pred_mat18_10_03_08_35_03.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>setsum_nn</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>1024</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.3</td>\n",
       "      <td>512</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>0.71853</td>\n",
       "      <td>0.72099</td>\n",
       "      <td>0.71741</td>\n",
       "      <td>output/y_pred_mat18_10_03_09_00_59.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>setsum_nn</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>1024</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.3</td>\n",
       "      <td>512</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>0.72344</td>\n",
       "      <td>0.72629</td>\n",
       "      <td>0.72262</td>\n",
       "      <td>output/y_pred_mat18_10_03_09_23_57.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>setsum_nn</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>1024</td>\n",
       "      <td>256</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.3</td>\n",
       "      <td>256</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>cosine</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>0.71379</td>\n",
       "      <td>0.71601</td>\n",
       "      <td>0.71339</td>\n",
       "      <td>output/y_pred_mat18_10_03_09_56_12.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>setsum_nn</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>1024</td>\n",
       "      <td>256</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.3</td>\n",
       "      <td>256</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>cosine</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>0.71009</td>\n",
       "      <td>0.71229</td>\n",
       "      <td>0.70927</td>\n",
       "      <td>output/y_pred_mat18_10_03_10_29_24.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>setsum_nn</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "      <td>256</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.3</td>\n",
       "      <td>256</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>cosine</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>0.71819</td>\n",
       "      <td>0.71988</td>\n",
       "      <td>0.71718</td>\n",
       "      <td>output/y_pred_mat18_10_03_11_00_11.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>setsum_nn</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "      <td>256</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.3</td>\n",
       "      <td>256</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>cosine</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>0.72016</td>\n",
       "      <td>0.72193</td>\n",
       "      <td>0.71890</td>\n",
       "      <td>output/y_pred_mat18_10_03_11_30_07.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>setsum_nn</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "      <td>256</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.3</td>\n",
       "      <td>256</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>cosine</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>0.72496</td>\n",
       "      <td>0.72680</td>\n",
       "      <td>0.72423</td>\n",
       "      <td>output/y_pred_mat18_10_03_11_59_46.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>setsum_nn</td>\n",
       "      <td>300</td>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.3</td>\n",
       "      <td>512</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>0.71500</td>\n",
       "      <td>0.71696</td>\n",
       "      <td>0.71463</td>\n",
       "      <td>output/y_pred_mat18_10_04_12_23_00.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>setsum_nn</td>\n",
       "      <td>300</td>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.3</td>\n",
       "      <td>512</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>0.71217</td>\n",
       "      <td>0.71411</td>\n",
       "      <td>0.71183</td>\n",
       "      <td>output/y_pred_mat18_10_04_12_45_58.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>setsum_nn</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>1024</td>\n",
       "      <td>256</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.3</td>\n",
       "      <td>256</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>cosine</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>0.71751</td>\n",
       "      <td>0.71936</td>\n",
       "      <td>0.71660</td>\n",
       "      <td>output/y_pred_mat18_10_04_01_14_27.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>setsum_nn</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>1024</td>\n",
       "      <td>256</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.3</td>\n",
       "      <td>256</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>cosine</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>0.71969</td>\n",
       "      <td>0.72185</td>\n",
       "      <td>0.71861</td>\n",
       "      <td>output/y_pred_mat18_10_04_01_44_52.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>setsum_nn</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>1024</td>\n",
       "      <td>256</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.3</td>\n",
       "      <td>256</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>cosine</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>0.72424</td>\n",
       "      <td>0.72625</td>\n",
       "      <td>0.72315</td>\n",
       "      <td>output/y_pred_mat18_10_04_02_14_31.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>setsum_nn</td>\n",
       "      <td>300</td>\n",
       "      <td>1</td>\n",
       "      <td>1024</td>\n",
       "      <td>256</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.3</td>\n",
       "      <td>256</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>0.71420</td>\n",
       "      <td>0.71593</td>\n",
       "      <td>0.71327</td>\n",
       "      <td>output/y_pred_mat18_10_04_02_45_39.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>setsum_nn</td>\n",
       "      <td>300</td>\n",
       "      <td>1</td>\n",
       "      <td>1024</td>\n",
       "      <td>256</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.3</td>\n",
       "      <td>256</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>0.71199</td>\n",
       "      <td>0.71367</td>\n",
       "      <td>0.71137</td>\n",
       "      <td>output/y_pred_mat18_10_04_03_18_59.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>setsum_nn</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.3</td>\n",
       "      <td>256</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>0.71698</td>\n",
       "      <td>0.71959</td>\n",
       "      <td>0.71534</td>\n",
       "      <td>output/y_pred_mat18_10_04_03_49_57.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>setsum_nn</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.3</td>\n",
       "      <td>256</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>0.71927</td>\n",
       "      <td>0.72210</td>\n",
       "      <td>0.71814</td>\n",
       "      <td>output/y_pred_mat18_10_04_04_23_49.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>setsum_nn</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.3</td>\n",
       "      <td>256</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>0.72331</td>\n",
       "      <td>0.72622</td>\n",
       "      <td>0.72231</td>\n",
       "      <td>output/y_pred_mat18_10_04_04_53_22.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>setsum_nn</td>\n",
       "      <td>300</td>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.3</td>\n",
       "      <td>512</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>cosine</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>0.71461</td>\n",
       "      <td>0.71644</td>\n",
       "      <td>0.71409</td>\n",
       "      <td>output/y_pred_mat18_10_04_05_16_38.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>setsum_nn</td>\n",
       "      <td>300</td>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.3</td>\n",
       "      <td>512</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>cosine</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>0.71123</td>\n",
       "      <td>0.71338</td>\n",
       "      <td>0.71102</td>\n",
       "      <td>output/y_pred_mat18_10_04_05_41_02.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>setsum_nn</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.3</td>\n",
       "      <td>256</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>0.72314</td>\n",
       "      <td>0.72572</td>\n",
       "      <td>0.72237</td>\n",
       "      <td>output/y_pred_mat18_10_04_05_08_46.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>setsum_nn</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.3</td>\n",
       "      <td>256</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>0.71225</td>\n",
       "      <td>0.71484</td>\n",
       "      <td>0.71148</td>\n",
       "      <td>output/y_pred_mat18_10_04_05_41_13.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>setsum_nn</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.3</td>\n",
       "      <td>256</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>0.72401</td>\n",
       "      <td>0.72693</td>\n",
       "      <td>0.72306</td>\n",
       "      <td>output/y_pred_mat18_10_04_06_11_25.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>setsum_nn</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.3</td>\n",
       "      <td>256</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>0.71677</td>\n",
       "      <td>0.71873</td>\n",
       "      <td>0.71533</td>\n",
       "      <td>output/y_pred_mat18_10_04_06_40_56.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>setsum_nn</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.3</td>\n",
       "      <td>256</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>0.71557</td>\n",
       "      <td>0.71755</td>\n",
       "      <td>0.71351</td>\n",
       "      <td>output/y_pred_mat18_10_04_07_10_32.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>setsum_nn</td>\n",
       "      <td>300</td>\n",
       "      <td>1</td>\n",
       "      <td>1024</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.3</td>\n",
       "      <td>512</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>cosine</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>0.72539</td>\n",
       "      <td>0.72684</td>\n",
       "      <td>0.72447</td>\n",
       "      <td>output/y_pred_mat18_10_04_07_35_02.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>setsum_nn</td>\n",
       "      <td>300</td>\n",
       "      <td>1</td>\n",
       "      <td>1024</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.3</td>\n",
       "      <td>512</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>cosine</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>0.71467</td>\n",
       "      <td>0.71645</td>\n",
       "      <td>0.71426</td>\n",
       "      <td>output/y_pred_mat18_10_04_07_59_16.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>setsum_nn</td>\n",
       "      <td>300</td>\n",
       "      <td>1</td>\n",
       "      <td>1024</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.3</td>\n",
       "      <td>512</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>cosine</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>0.72565</td>\n",
       "      <td>0.72755</td>\n",
       "      <td>0.72517</td>\n",
       "      <td>output/y_pred_mat18_10_04_08_22_34.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>setsum_nn</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>1024</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.3</td>\n",
       "      <td>256</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>0.71522</td>\n",
       "      <td>0.71784</td>\n",
       "      <td>0.71458</td>\n",
       "      <td>output/y_pred_mat18_10_04_08_55_09.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>setsum_nn</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>1024</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.3</td>\n",
       "      <td>256</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>0.71384</td>\n",
       "      <td>0.71651</td>\n",
       "      <td>0.71312</td>\n",
       "      <td>output/y_pred_mat18_10_04_09_25_49.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>setsum_nn</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.3</td>\n",
       "      <td>256</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>0.72579</td>\n",
       "      <td>0.72774</td>\n",
       "      <td>0.72474</td>\n",
       "      <td>output/y_pred_mat18_10_04_09_55_17.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>setsum_nn</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.3</td>\n",
       "      <td>256</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>0.71455</td>\n",
       "      <td>0.71633</td>\n",
       "      <td>0.71415</td>\n",
       "      <td>output/y_pred_mat18_10_04_10_27_56.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>setsum_nn</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.3</td>\n",
       "      <td>256</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>0.72636</td>\n",
       "      <td>0.72838</td>\n",
       "      <td>0.72565</td>\n",
       "      <td>output/y_pred_mat18_10_04_11_00_15.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>setsum_nn</td>\n",
       "      <td>300</td>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.3</td>\n",
       "      <td>256</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>cosine</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>0.71626</td>\n",
       "      <td>0.71819</td>\n",
       "      <td>0.71575</td>\n",
       "      <td>output/y_pred_mat18_10_04_11_30_56.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>setsum_nn</td>\n",
       "      <td>300</td>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.3</td>\n",
       "      <td>256</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>cosine</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>0.71488</td>\n",
       "      <td>0.71726</td>\n",
       "      <td>0.71456</td>\n",
       "      <td>output/y_pred_mat18_10_05_12_04_34.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>setsum_nn</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>1024</td>\n",
       "      <td>256</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.3</td>\n",
       "      <td>512</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>cosine</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>0.72396</td>\n",
       "      <td>0.72630</td>\n",
       "      <td>0.72379</td>\n",
       "      <td>output/y_pred_mat18_10_05_12_28_15.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>setsum_nn</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>1024</td>\n",
       "      <td>256</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.3</td>\n",
       "      <td>512</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>cosine</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>0.71332</td>\n",
       "      <td>0.71520</td>\n",
       "      <td>0.71174</td>\n",
       "      <td>output/y_pred_mat18_10_05_12_53_17.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>setsum_nn</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>1024</td>\n",
       "      <td>256</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.3</td>\n",
       "      <td>512</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>cosine</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>0.72563</td>\n",
       "      <td>0.72804</td>\n",
       "      <td>0.72553</td>\n",
       "      <td>output/y_pred_mat18_10_05_01_17_47.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>setsum_nn</td>\n",
       "      <td>300</td>\n",
       "      <td>1</td>\n",
       "      <td>1024</td>\n",
       "      <td>256</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.3</td>\n",
       "      <td>512</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>cosine</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>0.71562</td>\n",
       "      <td>0.71742</td>\n",
       "      <td>0.71492</td>\n",
       "      <td>output/y_pred_mat18_10_05_01_41_18.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>setsum_nn</td>\n",
       "      <td>300</td>\n",
       "      <td>1</td>\n",
       "      <td>1024</td>\n",
       "      <td>256</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.3</td>\n",
       "      <td>512</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>cosine</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>0.71433</td>\n",
       "      <td>0.71656</td>\n",
       "      <td>0.71357</td>\n",
       "      <td>output/y_pred_mat18_10_05_02_07_30.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>setsum_nn</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.3</td>\n",
       "      <td>512</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>0.72298</td>\n",
       "      <td>0.72598</td>\n",
       "      <td>0.72267</td>\n",
       "      <td>output/y_pred_mat18_10_05_02_29_47.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>setsum_nn</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.3</td>\n",
       "      <td>512</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>0.71134</td>\n",
       "      <td>0.71424</td>\n",
       "      <td>0.71027</td>\n",
       "      <td>output/y_pred_mat18_10_05_02_52_45.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>setsum_nn</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.3</td>\n",
       "      <td>512</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>0.72385</td>\n",
       "      <td>0.72683</td>\n",
       "      <td>0.72324</td>\n",
       "      <td>output/y_pred_mat18_10_05_03_17_09.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>setsum_nn</td>\n",
       "      <td>300</td>\n",
       "      <td>1</td>\n",
       "      <td>1024</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.3</td>\n",
       "      <td>512</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>0.71721</td>\n",
       "      <td>0.71913</td>\n",
       "      <td>0.71654</td>\n",
       "      <td>output/y_pred_mat18_10_05_03_41_06.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>setsum_nn</td>\n",
       "      <td>300</td>\n",
       "      <td>1</td>\n",
       "      <td>1024</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.3</td>\n",
       "      <td>512</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>0.71574</td>\n",
       "      <td>0.71779</td>\n",
       "      <td>0.71498</td>\n",
       "      <td>output/y_pred_mat18_10_05_04_03_51.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325</th>\n",
       "      <td>setsum_nn</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "      <td>256</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.3</td>\n",
       "      <td>512</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>0.72365</td>\n",
       "      <td>0.72589</td>\n",
       "      <td>0.72420</td>\n",
       "      <td>output/y_pred_mat18_10_05_04_25_55.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>setsum_nn</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "      <td>256</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.3</td>\n",
       "      <td>512</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>0.71374</td>\n",
       "      <td>0.71567</td>\n",
       "      <td>0.71349</td>\n",
       "      <td>output/y_pred_mat18_10_05_04_50_06.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>setsum_nn</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "      <td>256</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.3</td>\n",
       "      <td>512</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>cosine</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>0.72543</td>\n",
       "      <td>0.72766</td>\n",
       "      <td>0.72451</td>\n",
       "      <td>output/y_pred_mat18_10_05_05_13_41.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>setsum_nn</td>\n",
       "      <td>300</td>\n",
       "      <td>1</td>\n",
       "      <td>1024</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.3</td>\n",
       "      <td>512</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>cosine</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>0.71746</td>\n",
       "      <td>0.71926</td>\n",
       "      <td>0.71700</td>\n",
       "      <td>output/y_pred_mat18_10_05_05_38_41.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>setsum_nn</td>\n",
       "      <td>300</td>\n",
       "      <td>1</td>\n",
       "      <td>1024</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.3</td>\n",
       "      <td>512</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>cosine</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>0.71563</td>\n",
       "      <td>0.71758</td>\n",
       "      <td>0.71543</td>\n",
       "      <td>output/y_pred_mat18_10_05_06_02_06.npy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>330 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    model_name  code_embed_dim  hosp_embed_dim  fc_width  md_width     lr1  \\\n",
       "0    setsum_nn             200               1      1024       256  0.0002   \n",
       "1    setsum_nn             200               1      1024       256  0.0002   \n",
       "2    setsum_nn             200               1      1024       256  0.0002   \n",
       "3    setsum_nn             200               1       512       256  0.0002   \n",
       "4    setsum_nn             200               1       512       256  0.0002   \n",
       "5    setsum_nn             300               1      1024       256  0.0002   \n",
       "6    setsum_nn             300               1      1024       256  0.0002   \n",
       "7    setsum_nn             300               1      1024       256  0.0002   \n",
       "8    setsum_nn             300               1      1024       256  0.0002   \n",
       "9    setsum_nn             300               1      1024       256  0.0002   \n",
       "10   setsum_nn             200               1      1024       128  0.0002   \n",
       "11   setsum_nn             200               1      1024       128  0.0002   \n",
       "12   setsum_nn             200               1      1024       128  0.0002   \n",
       "13   setsum_nn             200               1      1024       256  0.0002   \n",
       "14   setsum_nn             200               1      1024       256  0.0002   \n",
       "15   setsum_nn             200               1       512       256  0.0002   \n",
       "16   setsum_nn             200               1       512       256  0.0002   \n",
       "17   setsum_nn             200               1       512       256  0.0002   \n",
       "18   setsum_nn             300               1       512       128  0.0002   \n",
       "19   setsum_nn             300               1       512       128  0.0002   \n",
       "20   setsum_nn             200               1      1024       256  0.0002   \n",
       "21   setsum_nn             200               1      1024       256  0.0002   \n",
       "22   setsum_nn             200               1      1024       256  0.0002   \n",
       "23   setsum_nn             300               1      1024       256  0.0002   \n",
       "24   setsum_nn             300               1      1024       256  0.0002   \n",
       "25   setsum_nn             200               1       512       128  0.0002   \n",
       "26   setsum_nn             200               1       512       128  0.0002   \n",
       "27   setsum_nn             200               1       512       128  0.0002   \n",
       "28   setsum_nn             300               1       512       128  0.0002   \n",
       "29   setsum_nn             300               1       512       128  0.0002   \n",
       "..         ...             ...             ...       ...       ...     ...   \n",
       "300  setsum_nn             200               1       512       128  0.0002   \n",
       "301  setsum_nn             200               1       512       128  0.0002   \n",
       "302  setsum_nn             200               1       512       128  0.0002   \n",
       "303  setsum_nn             200               1       512       128  0.0002   \n",
       "304  setsum_nn             200               1       512       128  0.0002   \n",
       "305  setsum_nn             300               1      1024       128  0.0002   \n",
       "306  setsum_nn             300               1      1024       128  0.0002   \n",
       "307  setsum_nn             300               1      1024       128  0.0002   \n",
       "308  setsum_nn             200               1      1024       128  0.0002   \n",
       "309  setsum_nn             200               1      1024       128  0.0002   \n",
       "310  setsum_nn             200               1       512       128  0.0002   \n",
       "311  setsum_nn             200               1       512       128  0.0002   \n",
       "312  setsum_nn             200               1       512       128  0.0002   \n",
       "313  setsum_nn             300               1       512       128  0.0002   \n",
       "314  setsum_nn             300               1       512       128  0.0002   \n",
       "315  setsum_nn             200               1      1024       256  0.0002   \n",
       "316  setsum_nn             200               1      1024       256  0.0002   \n",
       "317  setsum_nn             200               1      1024       256  0.0002   \n",
       "318  setsum_nn             300               1      1024       256  0.0002   \n",
       "319  setsum_nn             300               1      1024       256  0.0002   \n",
       "320  setsum_nn             200               1       512       128  0.0002   \n",
       "321  setsum_nn             200               1       512       128  0.0002   \n",
       "322  setsum_nn             200               1       512       128  0.0002   \n",
       "323  setsum_nn             300               1      1024       128  0.0002   \n",
       "324  setsum_nn             300               1      1024       128  0.0002   \n",
       "325  setsum_nn             200               1       512       256  0.0002   \n",
       "326  setsum_nn             200               1       512       256  0.0002   \n",
       "327  setsum_nn             200               1       512       256  0.0002   \n",
       "328  setsum_nn             300               1      1024       128  0.0002   \n",
       "329  setsum_nn             300               1      1024       128  0.0002   \n",
       "\n",
       "         lr2  dropout  batchsize embed_file  \\\n",
       "0    0.00002      0.3        256   pretrain   \n",
       "1    0.00002      0.3        256   pretrain   \n",
       "2    0.00002      0.3        256   pretrain   \n",
       "3    0.00002      0.3        256   pretrain   \n",
       "4    0.00002      0.3        256   pretrain   \n",
       "5    0.00002      0.3        512   pretrain   \n",
       "6    0.00002      0.3        512   pretrain   \n",
       "7    0.00002      0.3        512   pretrain   \n",
       "8    0.00002      0.3        512   pretrain   \n",
       "9    0.00002      0.3        512   pretrain   \n",
       "10   0.00002      0.3        512   pretrain   \n",
       "11   0.00002      0.3        512   pretrain   \n",
       "12   0.00002      0.3        512   pretrain   \n",
       "13   0.00002      0.3        256   pretrain   \n",
       "14   0.00002      0.3        256   pretrain   \n",
       "15   0.00002      0.3        256   pretrain   \n",
       "16   0.00002      0.3        256   pretrain   \n",
       "17   0.00002      0.3        256   pretrain   \n",
       "18   0.00002      0.3        512   pretrain   \n",
       "19   0.00002      0.3        512   pretrain   \n",
       "20   0.00002      0.3        256   pretrain   \n",
       "21   0.00002      0.3        256   pretrain   \n",
       "22   0.00002      0.3        256   pretrain   \n",
       "23   0.00002      0.3        256   pretrain   \n",
       "24   0.00002      0.3        256   pretrain   \n",
       "25   0.00002      0.3        256   pretrain   \n",
       "26   0.00002      0.3        256   pretrain   \n",
       "27   0.00002      0.3        256   pretrain   \n",
       "28   0.00002      0.3        512   pretrain   \n",
       "29   0.00002      0.3        512   pretrain   \n",
       "..       ...      ...        ...        ...   \n",
       "300  0.00002      0.3        256   pretrain   \n",
       "301  0.00002      0.3        256   pretrain   \n",
       "302  0.00002      0.3        256   pretrain   \n",
       "303  0.00002      0.3        256   pretrain   \n",
       "304  0.00002      0.3        256   pretrain   \n",
       "305  0.00002      0.3        512   pretrain   \n",
       "306  0.00002      0.3        512   pretrain   \n",
       "307  0.00002      0.3        512   pretrain   \n",
       "308  0.00002      0.3        256   pretrain   \n",
       "309  0.00002      0.3        256   pretrain   \n",
       "310  0.00002      0.3        256   pretrain   \n",
       "311  0.00002      0.3        256   pretrain   \n",
       "312  0.00002      0.3        256   pretrain   \n",
       "313  0.00002      0.3        256   pretrain   \n",
       "314  0.00002      0.3        256   pretrain   \n",
       "315  0.00002      0.3        512   pretrain   \n",
       "316  0.00002      0.3        512   pretrain   \n",
       "317  0.00002      0.3        512   pretrain   \n",
       "318  0.00002      0.3        512   pretrain   \n",
       "319  0.00002      0.3        512   pretrain   \n",
       "320  0.00002      0.3        512   pretrain   \n",
       "321  0.00002      0.3        512   pretrain   \n",
       "322  0.00002      0.3        512   pretrain   \n",
       "323  0.00002      0.3        512   pretrain   \n",
       "324  0.00002      0.3        512   pretrain   \n",
       "325  0.00002      0.3        512   pretrain   \n",
       "326  0.00002      0.3        512   pretrain   \n",
       "327  0.00002      0.3        512   pretrain   \n",
       "328  0.00002      0.3        512   pretrain   \n",
       "329  0.00002      0.3        512   pretrain   \n",
       "\n",
       "                      ...                   n_fold  penalty  penalty_metric  \\\n",
       "0                     ...                        5      1.0          cosine   \n",
       "1                     ...                        5      1.0          cosine   \n",
       "2                     ...                        5      1.0          cosine   \n",
       "3                     ...                        5      0.0          cosine   \n",
       "4                     ...                        5      0.0          cosine   \n",
       "5                     ...                        5      0.0          cosine   \n",
       "6                     ...                        5      0.0          cosine   \n",
       "7                     ...                        5      0.0          cosine   \n",
       "8                     ...                        5      0.0          cosine   \n",
       "9                     ...                        5      0.0          cosine   \n",
       "10                    ...                        5      0.0          cosine   \n",
       "11                    ...                        5      0.0          cosine   \n",
       "12                    ...                        5      0.0          cosine   \n",
       "13                    ...                        5      0.5          cosine   \n",
       "14                    ...                        5      0.5          cosine   \n",
       "15                    ...                        5      0.5          cosine   \n",
       "16                    ...                        5      0.5          cosine   \n",
       "17                    ...                        5      0.5          cosine   \n",
       "18                    ...                        5      1.0          cosine   \n",
       "19                    ...                        5      1.0          cosine   \n",
       "20                    ...                        5      0.5          cosine   \n",
       "21                    ...                        5      0.5          cosine   \n",
       "22                    ...                        5      0.5          cosine   \n",
       "23                    ...                        5      1.0          cosine   \n",
       "24                    ...                        5      1.0          cosine   \n",
       "25                    ...                        5      0.0          cosine   \n",
       "26                    ...                        5      0.0          cosine   \n",
       "27                    ...                        5      0.0          cosine   \n",
       "28                    ...                        5      0.5          cosine   \n",
       "29                    ...                        5      0.5          cosine   \n",
       "..                    ...                      ...      ...             ...   \n",
       "300                   ...                        5      0.0          cosine   \n",
       "301                   ...                        5      0.0          cosine   \n",
       "302                   ...                        5      0.0          cosine   \n",
       "303                   ...                        5      1.0          cosine   \n",
       "304                   ...                        5      1.0          cosine   \n",
       "305                   ...                        5      0.5          cosine   \n",
       "306                   ...                        5      0.5          cosine   \n",
       "307                   ...                        5      0.5          cosine   \n",
       "308                   ...                        5      0.0          cosine   \n",
       "309                   ...                        5      0.0          cosine   \n",
       "310                   ...                        5      1.0          cosine   \n",
       "311                   ...                        5      1.0          cosine   \n",
       "312                   ...                        5      1.0          cosine   \n",
       "313                   ...                        5      0.5          cosine   \n",
       "314                   ...                        5      0.5          cosine   \n",
       "315                   ...                        5      0.5          cosine   \n",
       "316                   ...                        5      0.5          cosine   \n",
       "317                   ...                        5      0.5          cosine   \n",
       "318                   ...                        5      0.5          cosine   \n",
       "319                   ...                        5      0.5          cosine   \n",
       "320                   ...                        5      0.0          cosine   \n",
       "321                   ...                        5      0.0          cosine   \n",
       "322                   ...                        5      0.0          cosine   \n",
       "323                   ...                        5      1.0          cosine   \n",
       "324                   ...                        5      1.0          cosine   \n",
       "325                   ...                        5      1.0          cosine   \n",
       "326                   ...                        5      1.0          cosine   \n",
       "327                   ...                        5      1.0          cosine   \n",
       "328                   ...                        5      0.5          cosine   \n",
       "329                   ...                        5      0.5          cosine   \n",
       "\n",
       "     count_cap DX_rarecutpoint  PR_rarecutpoint  auc_mean  auc_avg  \\\n",
       "0            5              20               10   0.71854  0.72020   \n",
       "1            5              20               10   0.71977  0.72173   \n",
       "2            5              20               10   0.72522  0.72696   \n",
       "3            5              20               10   0.71286  0.71549   \n",
       "4            5              20               10   0.70873  0.71141   \n",
       "5            0              20               10   0.71711  0.71959   \n",
       "6            0              20               10   0.71793  0.72071   \n",
       "7            0              20               10   0.72293  0.72548   \n",
       "8            0              20               10   0.71236  0.71453   \n",
       "9            0              20               10   0.70821  0.71076   \n",
       "10           5              20               10   0.71723  0.72062   \n",
       "11           5              20               10   0.71853  0.72099   \n",
       "12           5              20               10   0.72344  0.72629   \n",
       "13           0              20               10   0.71379  0.71601   \n",
       "14           0              20               10   0.71009  0.71229   \n",
       "15          20              20               10   0.71819  0.71988   \n",
       "16          20              20               10   0.72016  0.72193   \n",
       "17          20              20               10   0.72496  0.72680   \n",
       "18           5              20               10   0.71500  0.71696   \n",
       "19           5              20               10   0.71217  0.71411   \n",
       "20           5              20               10   0.71751  0.71936   \n",
       "21           5              20               10   0.71969  0.72185   \n",
       "22           5              20               10   0.72424  0.72625   \n",
       "23           5              20               10   0.71420  0.71593   \n",
       "24           5              20               10   0.71199  0.71367   \n",
       "25          20              20               10   0.71698  0.71959   \n",
       "26          20              20               10   0.71927  0.72210   \n",
       "27          20              20               10   0.72331  0.72622   \n",
       "28          20              20               10   0.71461  0.71644   \n",
       "29          20              20               10   0.71123  0.71338   \n",
       "..         ...             ...              ...       ...      ...   \n",
       "300         20              20               10   0.72314  0.72572   \n",
       "301         20              20               10   0.71225  0.71484   \n",
       "302         20              20               10   0.72401  0.72693   \n",
       "303         20              20               10   0.71677  0.71873   \n",
       "304         20              20               10   0.71557  0.71755   \n",
       "305         20              20               10   0.72539  0.72684   \n",
       "306         20              20               10   0.71467  0.71645   \n",
       "307         20              20               10   0.72565  0.72755   \n",
       "308          5              20               10   0.71522  0.71784   \n",
       "309          5              20               10   0.71384  0.71651   \n",
       "310          5              20               10   0.72579  0.72774   \n",
       "311          5              20               10   0.71455  0.71633   \n",
       "312          5              20               10   0.72636  0.72838   \n",
       "313          5              20               10   0.71626  0.71819   \n",
       "314          5              20               10   0.71488  0.71726   \n",
       "315          0              20               10   0.72396  0.72630   \n",
       "316          0              20               10   0.71332  0.71520   \n",
       "317          0              20               10   0.72563  0.72804   \n",
       "318          5              20               10   0.71562  0.71742   \n",
       "319          5              20               10   0.71433  0.71656   \n",
       "320          0              20               10   0.72298  0.72598   \n",
       "321          0              20               10   0.71134  0.71424   \n",
       "322          0              20               10   0.72385  0.72683   \n",
       "323         20              20               10   0.71721  0.71913   \n",
       "324         20              20               10   0.71574  0.71779   \n",
       "325          0              20               10   0.72365  0.72589   \n",
       "326          0              20               10   0.71374  0.71567   \n",
       "327          0              20               10   0.72543  0.72766   \n",
       "328         20              20               10   0.71746  0.71926   \n",
       "329         20              20               10   0.71563  0.71758   \n",
       "\n",
       "     auc_freeze                             y_pred_file  \n",
       "0       0.71711  output/y_pred_mat18_10_03_04_03_11.npy  \n",
       "1       0.71875  output/y_pred_mat18_10_03_04_32_18.npy  \n",
       "2       0.72325  output/y_pred_mat18_10_03_05_02_18.npy  \n",
       "3       0.71256  output/y_pred_mat18_10_03_05_34_03.npy  \n",
       "4       0.70968  output/y_pred_mat18_10_03_06_07_20.npy  \n",
       "5       0.71640  output/y_pred_mat18_10_03_06_30_57.npy  \n",
       "6       0.71688  output/y_pred_mat18_10_03_06_56_58.npy  \n",
       "7       0.72196  output/y_pred_mat18_10_03_07_22_56.npy  \n",
       "8       0.71221  output/y_pred_mat18_10_03_07_46_51.npy  \n",
       "9       0.70844  output/y_pred_mat18_10_03_08_11_43.npy  \n",
       "10      0.71692  output/y_pred_mat18_10_03_08_35_03.npy  \n",
       "11      0.71741  output/y_pred_mat18_10_03_09_00_59.npy  \n",
       "12      0.72262  output/y_pred_mat18_10_03_09_23_57.npy  \n",
       "13      0.71339  output/y_pred_mat18_10_03_09_56_12.npy  \n",
       "14      0.70927  output/y_pred_mat18_10_03_10_29_24.npy  \n",
       "15      0.71718  output/y_pred_mat18_10_03_11_00_11.npy  \n",
       "16      0.71890  output/y_pred_mat18_10_03_11_30_07.npy  \n",
       "17      0.72423  output/y_pred_mat18_10_03_11_59_46.npy  \n",
       "18      0.71463  output/y_pred_mat18_10_04_12_23_00.npy  \n",
       "19      0.71183  output/y_pred_mat18_10_04_12_45_58.npy  \n",
       "20      0.71660  output/y_pred_mat18_10_04_01_14_27.npy  \n",
       "21      0.71861  output/y_pred_mat18_10_04_01_44_52.npy  \n",
       "22      0.72315  output/y_pred_mat18_10_04_02_14_31.npy  \n",
       "23      0.71327  output/y_pred_mat18_10_04_02_45_39.npy  \n",
       "24      0.71137  output/y_pred_mat18_10_04_03_18_59.npy  \n",
       "25      0.71534  output/y_pred_mat18_10_04_03_49_57.npy  \n",
       "26      0.71814  output/y_pred_mat18_10_04_04_23_49.npy  \n",
       "27      0.72231  output/y_pred_mat18_10_04_04_53_22.npy  \n",
       "28      0.71409  output/y_pred_mat18_10_04_05_16_38.npy  \n",
       "29      0.71102  output/y_pred_mat18_10_04_05_41_02.npy  \n",
       "..          ...                                     ...  \n",
       "300     0.72237  output/y_pred_mat18_10_04_05_08_46.npy  \n",
       "301     0.71148  output/y_pred_mat18_10_04_05_41_13.npy  \n",
       "302     0.72306  output/y_pred_mat18_10_04_06_11_25.npy  \n",
       "303     0.71533  output/y_pred_mat18_10_04_06_40_56.npy  \n",
       "304     0.71351  output/y_pred_mat18_10_04_07_10_32.npy  \n",
       "305     0.72447  output/y_pred_mat18_10_04_07_35_02.npy  \n",
       "306     0.71426  output/y_pred_mat18_10_04_07_59_16.npy  \n",
       "307     0.72517  output/y_pred_mat18_10_04_08_22_34.npy  \n",
       "308     0.71458  output/y_pred_mat18_10_04_08_55_09.npy  \n",
       "309     0.71312  output/y_pred_mat18_10_04_09_25_49.npy  \n",
       "310     0.72474  output/y_pred_mat18_10_04_09_55_17.npy  \n",
       "311     0.71415  output/y_pred_mat18_10_04_10_27_56.npy  \n",
       "312     0.72565  output/y_pred_mat18_10_04_11_00_15.npy  \n",
       "313     0.71575  output/y_pred_mat18_10_04_11_30_56.npy  \n",
       "314     0.71456  output/y_pred_mat18_10_05_12_04_34.npy  \n",
       "315     0.72379  output/y_pred_mat18_10_05_12_28_15.npy  \n",
       "316     0.71174  output/y_pred_mat18_10_05_12_53_17.npy  \n",
       "317     0.72553  output/y_pred_mat18_10_05_01_17_47.npy  \n",
       "318     0.71492  output/y_pred_mat18_10_05_01_41_18.npy  \n",
       "319     0.71357  output/y_pred_mat18_10_05_02_07_30.npy  \n",
       "320     0.72267  output/y_pred_mat18_10_05_02_29_47.npy  \n",
       "321     0.71027  output/y_pred_mat18_10_05_02_52_45.npy  \n",
       "322     0.72324  output/y_pred_mat18_10_05_03_17_09.npy  \n",
       "323     0.71654  output/y_pred_mat18_10_05_03_41_06.npy  \n",
       "324     0.71498  output/y_pred_mat18_10_05_04_03_51.npy  \n",
       "325     0.72420  output/y_pred_mat18_10_05_04_25_55.npy  \n",
       "326     0.71349  output/y_pred_mat18_10_05_04_50_06.npy  \n",
       "327     0.72451  output/y_pred_mat18_10_05_05_13_41.npy  \n",
       "328     0.71700  output/y_pred_mat18_10_05_05_38_41.npy  \n",
       "329     0.71543  output/y_pred_mat18_10_05_06_02_06.npy  \n",
       "\n",
       "[330 rows x 22 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'code_embed_dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-051fe26bec99>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mres_grouped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'code_embed_dim'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fc_width'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'md_width'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'penalty'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'batchsize'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'count_cap'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/sw/lsa/centos7/python-anaconda-arc-connect/created-20170421/lib/python3.5/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mgroupby\u001b[0;34m(self, by, axis, level, as_index, sort, group_keys, squeeze, observed, **kwargs)\u001b[0m\n\u001b[1;32m   6663\u001b[0m         return groupby(self, by=by, axis=axis, level=level, as_index=as_index,\n\u001b[1;32m   6664\u001b[0m                        \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msqueeze\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6665\u001b[0;31m                        observed=observed, **kwargs)\n\u001b[0m\u001b[1;32m   6666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6667\u001b[0m     def asfreq(self, freq, method=None, how=None, normalize=False,\n",
      "\u001b[0;32m/sw/lsa/centos7/python-anaconda-arc-connect/created-20170421/lib/python3.5/site-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36mgroupby\u001b[0;34m(obj, by, **kwds)\u001b[0m\n\u001b[1;32m   2150\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'invalid type: %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2152\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mklass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mby\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/sw/lsa/centos7/python-anaconda-arc-connect/created-20170421/lib/python3.5/site-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, squeeze, observed, **kwargs)\u001b[0m\n\u001b[1;32m    597\u001b[0m                                                     \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m                                                     \u001b[0mobserved\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobserved\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m                                                     mutated=self.mutated)\n\u001b[0m\u001b[1;32m    600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/sw/lsa/centos7/python-anaconda-arc-connect/created-20170421/lib/python3.5/site-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36m_get_grouper\u001b[0;34m(obj, key, axis, level, sort, observed, mutated, validate)\u001b[0m\n\u001b[1;32m   3289\u001b[0m                 \u001b[0min_axis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3290\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3291\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3292\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGrouper\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mgpr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3293\u001b[0m             \u001b[0;31m# Add key to exclusions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'code_embed_dim'"
     ]
    }
   ],
   "source": [
    "res_grouped = res.groupby(['code_embed_dim', 'fc_width', 'md_width', 'penalty', 'batchsize', 'count_cap'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">auc_freeze</th>\n",
       "      <th colspan=\"2\" halign=\"left\">auc_mean</th>\n",
       "      <th colspan=\"2\" halign=\"left\">auc_avg</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>code_embed_dim</th>\n",
       "      <th>fc_width</th>\n",
       "      <th>md_width</th>\n",
       "      <th>penalty</th>\n",
       "      <th>batchsize</th>\n",
       "      <th>count_cap</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">200</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">512</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">128</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.0</th>\n",
       "      <th>256</th>\n",
       "      <th>20</th>\n",
       "      <td>0.716274</td>\n",
       "      <td>20</td>\n",
       "      <td>0.717040</td>\n",
       "      <td>20</td>\n",
       "      <td>0.719784</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>512</th>\n",
       "      <th>0</th>\n",
       "      <td>0.716043</td>\n",
       "      <td>20</td>\n",
       "      <td>0.716647</td>\n",
       "      <td>20</td>\n",
       "      <td>0.719599</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <th>0.0</th>\n",
       "      <th>256</th>\n",
       "      <th>5</th>\n",
       "      <td>0.716623</td>\n",
       "      <td>10</td>\n",
       "      <td>0.717226</td>\n",
       "      <td>10</td>\n",
       "      <td>0.719562</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">1024</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">128</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">0.0</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">256</th>\n",
       "      <th>5</th>\n",
       "      <td>0.716075</td>\n",
       "      <td>20</td>\n",
       "      <td>0.716861</td>\n",
       "      <td>20</td>\n",
       "      <td>0.719762</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.716289</td>\n",
       "      <td>10</td>\n",
       "      <td>0.717320</td>\n",
       "      <td>10</td>\n",
       "      <td>0.719926</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>512</th>\n",
       "      <th>5</th>\n",
       "      <td>0.716510</td>\n",
       "      <td>10</td>\n",
       "      <td>0.717119</td>\n",
       "      <td>10</td>\n",
       "      <td>0.720019</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <th>1024</th>\n",
       "      <th>256</th>\n",
       "      <th>0.0</th>\n",
       "      <th>512</th>\n",
       "      <th>0</th>\n",
       "      <td>0.715975</td>\n",
       "      <td>20</td>\n",
       "      <td>0.716324</td>\n",
       "      <td>20</td>\n",
       "      <td>0.718935</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                             auc_freeze        \\\n",
       "                                                                   mean count   \n",
       "code_embed_dim fc_width md_width penalty batchsize count_cap                    \n",
       "200            512      128      0.0     256       20          0.716274    20   \n",
       "                                         512       0           0.716043    20   \n",
       "                        256      0.0     256       5           0.716623    10   \n",
       "               1024     128      0.0     256       5           0.716075    20   \n",
       "                                                   20          0.716289    10   \n",
       "                                         512       5           0.716510    10   \n",
       "300            1024     256      0.0     512       0           0.715975    20   \n",
       "\n",
       "                                                              auc_mean        \\\n",
       "                                                                  mean count   \n",
       "code_embed_dim fc_width md_width penalty batchsize count_cap                   \n",
       "200            512      128      0.0     256       20         0.717040    20   \n",
       "                                         512       0          0.716647    20   \n",
       "                        256      0.0     256       5          0.717226    10   \n",
       "               1024     128      0.0     256       5          0.716861    20   \n",
       "                                                   20         0.717320    10   \n",
       "                                         512       5          0.717119    10   \n",
       "300            1024     256      0.0     512       0          0.716324    20   \n",
       "\n",
       "                                                               auc_avg        \n",
       "                                                                  mean count  \n",
       "code_embed_dim fc_width md_width penalty batchsize count_cap                  \n",
       "200            512      128      0.0     256       20         0.719784    20  \n",
       "                                         512       0          0.719599    20  \n",
       "                        256      0.0     256       5          0.719562    10  \n",
       "               1024     128      0.0     256       5          0.719762    20  \n",
       "                                                   20         0.719926    10  \n",
       "                                         512       5          0.720019    10  \n",
       "300            1024     256      0.0     512       0          0.718935    20  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_grouped[['auc_freeze', 'auc_mean', 'auc_avg']].agg(['mean', 'count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.to_csv('output/ht_result1003embed_nn_sub.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.read_csv('output/ht_result1003embed_nn_sub.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = res.loc[res.penalty==0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention with all codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for job_ind in range(11):\n",
    "    df = pd.read_csv('output/ht_result0212_'+str(job_ind)+'.csv', \n",
    "                     names=['model_name', 'code_embed_dim', 'hosp_embed_dim', 'fc_width', 'lr1', 'lr2', 'dropout',\n",
    "                            'batchsize', 'embed_file', 'cohort', 'tst_seed', 'n_fold', 'penalty', 'penalty_metric', 'count_cap', \n",
    "                            'DX1_rarecutpoint', 'DX_rarecutpoint', 'PR_rarecutpoint', 'other_pred', 'ndxpr', 'n_heads', \n",
    "                            'att_use_bias', 'att_activation', 'n_att_layers', 'auc_mean', 'auc_avg', 'auc_freeze', 'y_pred_file'], index_col=None)\n",
    "    res = pd.concat([res, df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>code_embed_dim</th>\n",
       "      <th>hosp_embed_dim</th>\n",
       "      <th>fc_width</th>\n",
       "      <th>lr1</th>\n",
       "      <th>lr2</th>\n",
       "      <th>dropout</th>\n",
       "      <th>batchsize</th>\n",
       "      <th>embed_file</th>\n",
       "      <th>cohort</th>\n",
       "      <th>...</th>\n",
       "      <th>other_pred</th>\n",
       "      <th>ndxpr</th>\n",
       "      <th>n_heads</th>\n",
       "      <th>att_use_bias</th>\n",
       "      <th>att_activation</th>\n",
       "      <th>n_att_layers</th>\n",
       "      <th>auc_mean</th>\n",
       "      <th>auc_avg</th>\n",
       "      <th>auc_freeze</th>\n",
       "      <th>y_pred_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>att_sum_nn</td>\n",
       "      <td>256</td>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>0.3</td>\n",
       "      <td>512</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>ami</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>linear</td>\n",
       "      <td>4</td>\n",
       "      <td>0.71408</td>\n",
       "      <td>0.71696</td>\n",
       "      <td>0.71206</td>\n",
       "      <td>/nfs/turbo/umms-awaljee/wsliu/Data/NRD/y_pred_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>att_sum_nn</td>\n",
       "      <td>256</td>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>0.3</td>\n",
       "      <td>512</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>chf</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>linear</td>\n",
       "      <td>4</td>\n",
       "      <td>0.62414</td>\n",
       "      <td>0.62818</td>\n",
       "      <td>0.62163</td>\n",
       "      <td>/nfs/turbo/umms-awaljee/wsliu/Data/NRD/y_pred_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>att_sum_nn</td>\n",
       "      <td>256</td>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>0.3</td>\n",
       "      <td>512</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>ami</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>linear</td>\n",
       "      <td>4</td>\n",
       "      <td>0.72172</td>\n",
       "      <td>0.72514</td>\n",
       "      <td>0.72092</td>\n",
       "      <td>/nfs/turbo/umms-awaljee/wsliu/Data/NRD/y_pred_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>att_sum_nn</td>\n",
       "      <td>256</td>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>0.3</td>\n",
       "      <td>512</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>chf</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>linear</td>\n",
       "      <td>4</td>\n",
       "      <td>0.63001</td>\n",
       "      <td>0.63475</td>\n",
       "      <td>0.62672</td>\n",
       "      <td>/nfs/turbo/umms-awaljee/wsliu/Data/NRD/y_pred_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>att_sum_nn</td>\n",
       "      <td>256</td>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>0.3</td>\n",
       "      <td>512</td>\n",
       "      <td>pretrain</td>\n",
       "      <td>pna</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>linear</td>\n",
       "      <td>4</td>\n",
       "      <td>0.67541</td>\n",
       "      <td>0.68059</td>\n",
       "      <td>0.67470</td>\n",
       "      <td>/nfs/turbo/umms-awaljee/wsliu/Data/NRD/y_pred_...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   model_name  code_embed_dim  hosp_embed_dim  fc_width     lr1      lr2  \\\n",
       "0  att_sum_nn             256               1       512  0.0002  0.00005   \n",
       "1  att_sum_nn             256               1       512  0.0002  0.00005   \n",
       "0  att_sum_nn             256               1       512  0.0002  0.00005   \n",
       "1  att_sum_nn             256               1       512  0.0002  0.00005   \n",
       "2  att_sum_nn             256               1       512  0.0002  0.00005   \n",
       "\n",
       "   dropout  batchsize embed_file cohort  \\\n",
       "0      0.3        512   pretrain    ami   \n",
       "1      0.3        512   pretrain    chf   \n",
       "0      0.3        512   pretrain    ami   \n",
       "1      0.3        512   pretrain    chf   \n",
       "2      0.3        512   pretrain    pna   \n",
       "\n",
       "                         ...                          other_pred  ndxpr  \\\n",
       "0                        ...                                   0      0   \n",
       "1                        ...                                   0      0   \n",
       "0                        ...                                   0      0   \n",
       "1                        ...                                   0      0   \n",
       "2                        ...                                   0      0   \n",
       "\n",
       "   n_heads att_use_bias  att_activation  n_att_layers  auc_mean  auc_avg  \\\n",
       "0       32            0          linear             4   0.71408  0.71696   \n",
       "1       32            0          linear             4   0.62414  0.62818   \n",
       "0       32            0          linear             4   0.72172  0.72514   \n",
       "1       32            0          linear             4   0.63001  0.63475   \n",
       "2       32            0          linear             4   0.67541  0.68059   \n",
       "\n",
       "   auc_freeze                                        y_pred_file  \n",
       "0     0.71206  /nfs/turbo/umms-awaljee/wsliu/Data/NRD/y_pred_...  \n",
       "1     0.62163  /nfs/turbo/umms-awaljee/wsliu/Data/NRD/y_pred_...  \n",
       "0     0.72092  /nfs/turbo/umms-awaljee/wsliu/Data/NRD/y_pred_...  \n",
       "1     0.62672  /nfs/turbo/umms-awaljee/wsliu/Data/NRD/y_pred_...  \n",
       "2     0.67470  /nfs/turbo/umms-awaljee/wsliu/Data/NRD/y_pred_...  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'code_embed_dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-161319a4cafa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgrouped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'code_embed_dim'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'n_att_layers'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'n_heads'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'penalty'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'att_use_bias'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/sw/lsa/centos7/python-anaconda-arc-connect/created-20170421/lib/python3.5/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mgroupby\u001b[0;34m(self, by, axis, level, as_index, sort, group_keys, squeeze, observed, **kwargs)\u001b[0m\n\u001b[1;32m   6663\u001b[0m         return groupby(self, by=by, axis=axis, level=level, as_index=as_index,\n\u001b[1;32m   6664\u001b[0m                        \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msqueeze\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6665\u001b[0;31m                        observed=observed, **kwargs)\n\u001b[0m\u001b[1;32m   6666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6667\u001b[0m     def asfreq(self, freq, method=None, how=None, normalize=False,\n",
      "\u001b[0;32m/sw/lsa/centos7/python-anaconda-arc-connect/created-20170421/lib/python3.5/site-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36mgroupby\u001b[0;34m(obj, by, **kwds)\u001b[0m\n\u001b[1;32m   2150\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'invalid type: %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2152\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mklass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mby\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/sw/lsa/centos7/python-anaconda-arc-connect/created-20170421/lib/python3.5/site-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, squeeze, observed, **kwargs)\u001b[0m\n\u001b[1;32m    597\u001b[0m                                                     \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m                                                     \u001b[0mobserved\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobserved\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m                                                     mutated=self.mutated)\n\u001b[0m\u001b[1;32m    600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/sw/lsa/centos7/python-anaconda-arc-connect/created-20170421/lib/python3.5/site-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36m_get_grouper\u001b[0;34m(obj, key, axis, level, sort, observed, mutated, validate)\u001b[0m\n\u001b[1;32m   3289\u001b[0m                 \u001b[0min_axis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3290\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3291\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3292\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGrouper\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mgpr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3293\u001b[0m             \u001b[0;31m# Add key to exclusions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'code_embed_dim'"
     ]
    }
   ],
   "source": [
    "grouped = res.groupby(['model_name', 'code_embed_dim', 'n_att_layers', 'n_heads', 'penalty', 'att_use_bias'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">auc_freeze</th>\n",
       "      <th colspan=\"2\" halign=\"left\">auc_mean</th>\n",
       "      <th colspan=\"2\" halign=\"left\">auc_avg</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_name</th>\n",
       "      <th>code_embed_dim</th>\n",
       "      <th>n_att_layers</th>\n",
       "      <th>n_heads</th>\n",
       "      <th>penalty</th>\n",
       "      <th>att_use_bias</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"8\" valign=\"top\">att_shortcut_nn</th>\n",
       "      <th rowspan=\"8\" valign=\"top\">256</th>\n",
       "      <th rowspan=\"7\" valign=\"top\">6</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">8</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0</th>\n",
       "      <td>0.71202</td>\n",
       "      <td>1</td>\n",
       "      <td>0.71071</td>\n",
       "      <td>1</td>\n",
       "      <td>0.71530</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">1.0</th>\n",
       "      <th>0</th>\n",
       "      <td>0.71370</td>\n",
       "      <td>1</td>\n",
       "      <td>0.71435</td>\n",
       "      <td>1</td>\n",
       "      <td>0.71686</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.71410</td>\n",
       "      <td>1</td>\n",
       "      <td>0.71475</td>\n",
       "      <td>1</td>\n",
       "      <td>0.71735</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">16</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0</th>\n",
       "      <td>0.71273</td>\n",
       "      <td>1</td>\n",
       "      <td>0.71413</td>\n",
       "      <td>1</td>\n",
       "      <td>0.71719</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">1.0</th>\n",
       "      <th>0</th>\n",
       "      <td>0.71382</td>\n",
       "      <td>1</td>\n",
       "      <td>0.71405</td>\n",
       "      <td>1</td>\n",
       "      <td>0.71645</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.71326</td>\n",
       "      <td>1</td>\n",
       "      <td>0.71409</td>\n",
       "      <td>1</td>\n",
       "      <td>0.71684</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <th>1.0</th>\n",
       "      <th>0</th>\n",
       "      <td>0.71331</td>\n",
       "      <td>1</td>\n",
       "      <td>0.71363</td>\n",
       "      <td>1</td>\n",
       "      <td>0.71646</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <th>8</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0</th>\n",
       "      <td>0.70662</td>\n",
       "      <td>1</td>\n",
       "      <td>0.71049</td>\n",
       "      <td>1</td>\n",
       "      <td>0.71299</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"26\" valign=\"top\">att_sum_nn</th>\n",
       "      <th rowspan=\"16\" valign=\"top\">256</th>\n",
       "      <th rowspan=\"6\" valign=\"top\">2</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">8</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0</th>\n",
       "      <td>0.71777</td>\n",
       "      <td>1</td>\n",
       "      <td>0.71904</td>\n",
       "      <td>1</td>\n",
       "      <td>0.72123</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <th>0</th>\n",
       "      <td>0.71738</td>\n",
       "      <td>1</td>\n",
       "      <td>0.71881</td>\n",
       "      <td>1</td>\n",
       "      <td>0.72063</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">16</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0</th>\n",
       "      <td>0.71710</td>\n",
       "      <td>1</td>\n",
       "      <td>0.71838</td>\n",
       "      <td>1</td>\n",
       "      <td>0.72107</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <th>0</th>\n",
       "      <td>0.71749</td>\n",
       "      <td>1</td>\n",
       "      <td>0.71862</td>\n",
       "      <td>1</td>\n",
       "      <td>0.72063</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">32</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0</th>\n",
       "      <td>0.71720</td>\n",
       "      <td>1</td>\n",
       "      <td>0.71865</td>\n",
       "      <td>1</td>\n",
       "      <td>0.72080</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <th>0</th>\n",
       "      <td>0.71761</td>\n",
       "      <td>1</td>\n",
       "      <td>0.71913</td>\n",
       "      <td>1</td>\n",
       "      <td>0.72168</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">4</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">8</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0</th>\n",
       "      <td>0.71732</td>\n",
       "      <td>1</td>\n",
       "      <td>0.71822</td>\n",
       "      <td>1</td>\n",
       "      <td>0.72066</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <th>0</th>\n",
       "      <td>0.71659</td>\n",
       "      <td>1</td>\n",
       "      <td>0.71820</td>\n",
       "      <td>1</td>\n",
       "      <td>0.72038</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">16</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0</th>\n",
       "      <td>0.71680</td>\n",
       "      <td>1</td>\n",
       "      <td>0.71807</td>\n",
       "      <td>1</td>\n",
       "      <td>0.72130</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <th>0</th>\n",
       "      <td>0.71659</td>\n",
       "      <td>1</td>\n",
       "      <td>0.71852</td>\n",
       "      <td>1</td>\n",
       "      <td>0.72120</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">32</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0</th>\n",
       "      <td>0.71778</td>\n",
       "      <td>1</td>\n",
       "      <td>0.71950</td>\n",
       "      <td>1</td>\n",
       "      <td>0.72180</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <th>0</th>\n",
       "      <td>0.71714</td>\n",
       "      <td>1</td>\n",
       "      <td>0.71861</td>\n",
       "      <td>1</td>\n",
       "      <td>0.72085</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">8</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">8</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0</th>\n",
       "      <td>0.71630</td>\n",
       "      <td>1</td>\n",
       "      <td>0.71774</td>\n",
       "      <td>1</td>\n",
       "      <td>0.72045</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <th>0</th>\n",
       "      <td>0.71565</td>\n",
       "      <td>1</td>\n",
       "      <td>0.71810</td>\n",
       "      <td>1</td>\n",
       "      <td>0.72095</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">16</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0</th>\n",
       "      <td>0.71592</td>\n",
       "      <td>1</td>\n",
       "      <td>0.71754</td>\n",
       "      <td>1</td>\n",
       "      <td>0.72006</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <th>0</th>\n",
       "      <td>0.71626</td>\n",
       "      <td>1</td>\n",
       "      <td>0.71836</td>\n",
       "      <td>1</td>\n",
       "      <td>0.72049</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"10\" valign=\"top\">512</th>\n",
       "      <th rowspan=\"6\" valign=\"top\">2</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">8</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0</th>\n",
       "      <td>0.71692</td>\n",
       "      <td>1</td>\n",
       "      <td>0.71904</td>\n",
       "      <td>1</td>\n",
       "      <td>0.72073</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <th>0</th>\n",
       "      <td>0.71804</td>\n",
       "      <td>1</td>\n",
       "      <td>0.71858</td>\n",
       "      <td>1</td>\n",
       "      <td>0.72063</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">16</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0</th>\n",
       "      <td>0.71697</td>\n",
       "      <td>1</td>\n",
       "      <td>0.71863</td>\n",
       "      <td>1</td>\n",
       "      <td>0.72070</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <th>0</th>\n",
       "      <td>0.71665</td>\n",
       "      <td>1</td>\n",
       "      <td>0.71856</td>\n",
       "      <td>1</td>\n",
       "      <td>0.72078</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">32</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0</th>\n",
       "      <td>0.71683</td>\n",
       "      <td>1</td>\n",
       "      <td>0.71840</td>\n",
       "      <td>1</td>\n",
       "      <td>0.72044</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <th>0</th>\n",
       "      <td>0.71850</td>\n",
       "      <td>1</td>\n",
       "      <td>0.71928</td>\n",
       "      <td>1</td>\n",
       "      <td>0.72168</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">4</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">8</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0</th>\n",
       "      <td>0.71599</td>\n",
       "      <td>1</td>\n",
       "      <td>0.71689</td>\n",
       "      <td>1</td>\n",
       "      <td>0.71971</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <th>0</th>\n",
       "      <td>0.71565</td>\n",
       "      <td>1</td>\n",
       "      <td>0.71655</td>\n",
       "      <td>1</td>\n",
       "      <td>0.71979</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <th>1.0</th>\n",
       "      <th>0</th>\n",
       "      <td>0.71702</td>\n",
       "      <td>1</td>\n",
       "      <td>0.71794</td>\n",
       "      <td>1</td>\n",
       "      <td>0.72045</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <th>8</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0</th>\n",
       "      <td>0.71504</td>\n",
       "      <td>1</td>\n",
       "      <td>0.71695</td>\n",
       "      <td>1</td>\n",
       "      <td>0.71997</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                         auc_freeze  \\\n",
       "                                                                               mean   \n",
       "model_name      code_embed_dim n_att_layers n_heads penalty att_use_bias              \n",
       "att_shortcut_nn 256            6            8       0.0     0               0.71202   \n",
       "                                                    1.0     0               0.71370   \n",
       "                                                            1               0.71410   \n",
       "                                            16      0.0     0               0.71273   \n",
       "                                                    1.0     0               0.71382   \n",
       "                                                            1               0.71326   \n",
       "                                            32      1.0     0               0.71331   \n",
       "                               12           8       0.0     0               0.70662   \n",
       "att_sum_nn      256            2            8       0.0     0               0.71777   \n",
       "                                                    1.0     0               0.71738   \n",
       "                                            16      0.0     0               0.71710   \n",
       "                                                    1.0     0               0.71749   \n",
       "                                            32      0.0     0               0.71720   \n",
       "                                                    1.0     0               0.71761   \n",
       "                               4            8       0.0     0               0.71732   \n",
       "                                                    1.0     0               0.71659   \n",
       "                                            16      0.0     0               0.71680   \n",
       "                                                    1.0     0               0.71659   \n",
       "                                            32      0.0     0               0.71778   \n",
       "                                                    1.0     0               0.71714   \n",
       "                               8            8       0.0     0               0.71630   \n",
       "                                                    1.0     0               0.71565   \n",
       "                                            16      0.0     0               0.71592   \n",
       "                                                    1.0     0               0.71626   \n",
       "                512            2            8       0.0     0               0.71692   \n",
       "                                                    1.0     0               0.71804   \n",
       "                                            16      0.0     0               0.71697   \n",
       "                                                    1.0     0               0.71665   \n",
       "                                            32      0.0     0               0.71683   \n",
       "                                                    1.0     0               0.71850   \n",
       "                               4            8       0.0     0               0.71599   \n",
       "                                                    1.0     0               0.71565   \n",
       "                                            16      1.0     0               0.71702   \n",
       "                               8            8       0.0     0               0.71504   \n",
       "\n",
       "                                                                                \\\n",
       "                                                                         count   \n",
       "model_name      code_embed_dim n_att_layers n_heads penalty att_use_bias         \n",
       "att_shortcut_nn 256            6            8       0.0     0                1   \n",
       "                                                    1.0     0                1   \n",
       "                                                            1                1   \n",
       "                                            16      0.0     0                1   \n",
       "                                                    1.0     0                1   \n",
       "                                                            1                1   \n",
       "                                            32      1.0     0                1   \n",
       "                               12           8       0.0     0                1   \n",
       "att_sum_nn      256            2            8       0.0     0                1   \n",
       "                                                    1.0     0                1   \n",
       "                                            16      0.0     0                1   \n",
       "                                                    1.0     0                1   \n",
       "                                            32      0.0     0                1   \n",
       "                                                    1.0     0                1   \n",
       "                               4            8       0.0     0                1   \n",
       "                                                    1.0     0                1   \n",
       "                                            16      0.0     0                1   \n",
       "                                                    1.0     0                1   \n",
       "                                            32      0.0     0                1   \n",
       "                                                    1.0     0                1   \n",
       "                               8            8       0.0     0                1   \n",
       "                                                    1.0     0                1   \n",
       "                                            16      0.0     0                1   \n",
       "                                                    1.0     0                1   \n",
       "                512            2            8       0.0     0                1   \n",
       "                                                    1.0     0                1   \n",
       "                                            16      0.0     0                1   \n",
       "                                                    1.0     0                1   \n",
       "                                            32      0.0     0                1   \n",
       "                                                    1.0     0                1   \n",
       "                               4            8       0.0     0                1   \n",
       "                                                    1.0     0                1   \n",
       "                                            16      1.0     0                1   \n",
       "                               8            8       0.0     0                1   \n",
       "\n",
       "                                                                         auc_mean  \\\n",
       "                                                                             mean   \n",
       "model_name      code_embed_dim n_att_layers n_heads penalty att_use_bias            \n",
       "att_shortcut_nn 256            6            8       0.0     0             0.71071   \n",
       "                                                    1.0     0             0.71435   \n",
       "                                                            1             0.71475   \n",
       "                                            16      0.0     0             0.71413   \n",
       "                                                    1.0     0             0.71405   \n",
       "                                                            1             0.71409   \n",
       "                                            32      1.0     0             0.71363   \n",
       "                               12           8       0.0     0             0.71049   \n",
       "att_sum_nn      256            2            8       0.0     0             0.71904   \n",
       "                                                    1.0     0             0.71881   \n",
       "                                            16      0.0     0             0.71838   \n",
       "                                                    1.0     0             0.71862   \n",
       "                                            32      0.0     0             0.71865   \n",
       "                                                    1.0     0             0.71913   \n",
       "                               4            8       0.0     0             0.71822   \n",
       "                                                    1.0     0             0.71820   \n",
       "                                            16      0.0     0             0.71807   \n",
       "                                                    1.0     0             0.71852   \n",
       "                                            32      0.0     0             0.71950   \n",
       "                                                    1.0     0             0.71861   \n",
       "                               8            8       0.0     0             0.71774   \n",
       "                                                    1.0     0             0.71810   \n",
       "                                            16      0.0     0             0.71754   \n",
       "                                                    1.0     0             0.71836   \n",
       "                512            2            8       0.0     0             0.71904   \n",
       "                                                    1.0     0             0.71858   \n",
       "                                            16      0.0     0             0.71863   \n",
       "                                                    1.0     0             0.71856   \n",
       "                                            32      0.0     0             0.71840   \n",
       "                                                    1.0     0             0.71928   \n",
       "                               4            8       0.0     0             0.71689   \n",
       "                                                    1.0     0             0.71655   \n",
       "                                            16      1.0     0             0.71794   \n",
       "                               8            8       0.0     0             0.71695   \n",
       "\n",
       "                                                                                \\\n",
       "                                                                         count   \n",
       "model_name      code_embed_dim n_att_layers n_heads penalty att_use_bias         \n",
       "att_shortcut_nn 256            6            8       0.0     0                1   \n",
       "                                                    1.0     0                1   \n",
       "                                                            1                1   \n",
       "                                            16      0.0     0                1   \n",
       "                                                    1.0     0                1   \n",
       "                                                            1                1   \n",
       "                                            32      1.0     0                1   \n",
       "                               12           8       0.0     0                1   \n",
       "att_sum_nn      256            2            8       0.0     0                1   \n",
       "                                                    1.0     0                1   \n",
       "                                            16      0.0     0                1   \n",
       "                                                    1.0     0                1   \n",
       "                                            32      0.0     0                1   \n",
       "                                                    1.0     0                1   \n",
       "                               4            8       0.0     0                1   \n",
       "                                                    1.0     0                1   \n",
       "                                            16      0.0     0                1   \n",
       "                                                    1.0     0                1   \n",
       "                                            32      0.0     0                1   \n",
       "                                                    1.0     0                1   \n",
       "                               8            8       0.0     0                1   \n",
       "                                                    1.0     0                1   \n",
       "                                            16      0.0     0                1   \n",
       "                                                    1.0     0                1   \n",
       "                512            2            8       0.0     0                1   \n",
       "                                                    1.0     0                1   \n",
       "                                            16      0.0     0                1   \n",
       "                                                    1.0     0                1   \n",
       "                                            32      0.0     0                1   \n",
       "                                                    1.0     0                1   \n",
       "                               4            8       0.0     0                1   \n",
       "                                                    1.0     0                1   \n",
       "                                            16      1.0     0                1   \n",
       "                               8            8       0.0     0                1   \n",
       "\n",
       "                                                                          auc_avg  \\\n",
       "                                                                             mean   \n",
       "model_name      code_embed_dim n_att_layers n_heads penalty att_use_bias            \n",
       "att_shortcut_nn 256            6            8       0.0     0             0.71530   \n",
       "                                                    1.0     0             0.71686   \n",
       "                                                            1             0.71735   \n",
       "                                            16      0.0     0             0.71719   \n",
       "                                                    1.0     0             0.71645   \n",
       "                                                            1             0.71684   \n",
       "                                            32      1.0     0             0.71646   \n",
       "                               12           8       0.0     0             0.71299   \n",
       "att_sum_nn      256            2            8       0.0     0             0.72123   \n",
       "                                                    1.0     0             0.72063   \n",
       "                                            16      0.0     0             0.72107   \n",
       "                                                    1.0     0             0.72063   \n",
       "                                            32      0.0     0             0.72080   \n",
       "                                                    1.0     0             0.72168   \n",
       "                               4            8       0.0     0             0.72066   \n",
       "                                                    1.0     0             0.72038   \n",
       "                                            16      0.0     0             0.72130   \n",
       "                                                    1.0     0             0.72120   \n",
       "                                            32      0.0     0             0.72180   \n",
       "                                                    1.0     0             0.72085   \n",
       "                               8            8       0.0     0             0.72045   \n",
       "                                                    1.0     0             0.72095   \n",
       "                                            16      0.0     0             0.72006   \n",
       "                                                    1.0     0             0.72049   \n",
       "                512            2            8       0.0     0             0.72073   \n",
       "                                                    1.0     0             0.72063   \n",
       "                                            16      0.0     0             0.72070   \n",
       "                                                    1.0     0             0.72078   \n",
       "                                            32      0.0     0             0.72044   \n",
       "                                                    1.0     0             0.72168   \n",
       "                               4            8       0.0     0             0.71971   \n",
       "                                                    1.0     0             0.71979   \n",
       "                                            16      1.0     0             0.72045   \n",
       "                               8            8       0.0     0             0.71997   \n",
       "\n",
       "                                                                                \n",
       "                                                                         count  \n",
       "model_name      code_embed_dim n_att_layers n_heads penalty att_use_bias        \n",
       "att_shortcut_nn 256            6            8       0.0     0                1  \n",
       "                                                    1.0     0                1  \n",
       "                                                            1                1  \n",
       "                                            16      0.0     0                1  \n",
       "                                                    1.0     0                1  \n",
       "                                                            1                1  \n",
       "                                            32      1.0     0                1  \n",
       "                               12           8       0.0     0                1  \n",
       "att_sum_nn      256            2            8       0.0     0                1  \n",
       "                                                    1.0     0                1  \n",
       "                                            16      0.0     0                1  \n",
       "                                                    1.0     0                1  \n",
       "                                            32      0.0     0                1  \n",
       "                                                    1.0     0                1  \n",
       "                               4            8       0.0     0                1  \n",
       "                                                    1.0     0                1  \n",
       "                                            16      0.0     0                1  \n",
       "                                                    1.0     0                1  \n",
       "                                            32      0.0     0                1  \n",
       "                                                    1.0     0                1  \n",
       "                               8            8       0.0     0                1  \n",
       "                                                    1.0     0                1  \n",
       "                                            16      0.0     0                1  \n",
       "                                                    1.0     0                1  \n",
       "                512            2            8       0.0     0                1  \n",
       "                                                    1.0     0                1  \n",
       "                                            16      0.0     0                1  \n",
       "                                                    1.0     0                1  \n",
       "                                            32      0.0     0                1  \n",
       "                                                    1.0     0                1  \n",
       "                               4            8       0.0     0                1  \n",
       "                                                    1.0     0                1  \n",
       "                                            16      1.0     0                1  \n",
       "                               8            8       0.0     0                1  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped[['auc_freeze', 'auc_mean', 'auc_avg']].agg(['mean', 'count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['/nfs/turbo/umms-awaljee/wsliu/Data/NRD/y_pred_mat/y_pred_mat19_02_06_09_09_36.npy',\n",
       "       '/nfs/turbo/umms-awaljee/wsliu/Data/NRD/y_pred_mat/y_pred_mat19_02_06_08_07_19.npy'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.loc[(res.model_name=='att_lr') & (res.code_embed_dim==256) & (res.n_heads==8), 'y_pred_file'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.to_csv('output/ht_result0212.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">auc_freeze</th>\n",
       "      <th colspan=\"2\" halign=\"left\">auc_mean</th>\n",
       "      <th colspan=\"2\" halign=\"left\">auc_avg</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cohort</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ami</th>\n",
       "      <td>0.714064</td>\n",
       "      <td>10</td>\n",
       "      <td>0.715065</td>\n",
       "      <td>10</td>\n",
       "      <td>0.718193</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chf</th>\n",
       "      <td>0.623843</td>\n",
       "      <td>10</td>\n",
       "      <td>0.625360</td>\n",
       "      <td>10</td>\n",
       "      <td>0.630041</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pna</th>\n",
       "      <td>0.670690</td>\n",
       "      <td>4</td>\n",
       "      <td>0.672050</td>\n",
       "      <td>4</td>\n",
       "      <td>0.676370</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       auc_freeze        auc_mean         auc_avg      \n",
       "             mean count      mean count      mean count\n",
       "cohort                                                 \n",
       "ami      0.714064    10  0.715065    10  0.718193    10\n",
       "chf      0.623843    10  0.625360    10  0.630041    10\n",
       "pna      0.670690     4  0.672050     4  0.676370     4"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.groupby('cohort')[['auc_freeze', 'auc_mean', 'auc_avg']].agg(['mean', 'count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.read_csv('output/ht_result0125.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OHE models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "for job_ind in range(1):\n",
    "    df = pd.read_csv('output/ht_result0222_'+str(job_ind)+'.csv', \n",
    "                     names=['fc_width1', 'fc_width2', 'lr', 'dropout', 'batchsize', 'cohort', 'tst_seed', 'n_fold', \n",
    "                            'DX_rarecutpoint', 'PR_rarecutpoint', 'auc_mean', 'auc_avg', 'y_pred_file'], index_col=None)\n",
    "    res = pd.concat([res, df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fc_width1</th>\n",
       "      <th>fc_width2</th>\n",
       "      <th>lr</th>\n",
       "      <th>dropout</th>\n",
       "      <th>batchsize</th>\n",
       "      <th>cohort</th>\n",
       "      <th>tst_seed</th>\n",
       "      <th>n_fold</th>\n",
       "      <th>DX_rarecutpoint</th>\n",
       "      <th>PR_rarecutpoint</th>\n",
       "      <th>auc_mean</th>\n",
       "      <th>auc_avg</th>\n",
       "      <th>y_pred_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1024</td>\n",
       "      <td>256</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.3</td>\n",
       "      <td>512</td>\n",
       "      <td>ami</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0.66806</td>\n",
       "      <td>0.67238</td>\n",
       "      <td>/nfs/turbo/umms-awaljee/wsliu/Data/NRD/y_pred_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1024</td>\n",
       "      <td>256</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.3</td>\n",
       "      <td>512</td>\n",
       "      <td>chf</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0.60358</td>\n",
       "      <td>0.60744</td>\n",
       "      <td>/nfs/turbo/umms-awaljee/wsliu/Data/NRD/y_pred_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1024</td>\n",
       "      <td>256</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.3</td>\n",
       "      <td>512</td>\n",
       "      <td>pna</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0.63760</td>\n",
       "      <td>0.64151</td>\n",
       "      <td>/nfs/turbo/umms-awaljee/wsliu/Data/NRD/y_pred_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1024</td>\n",
       "      <td>256</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.3</td>\n",
       "      <td>512</td>\n",
       "      <td>ami</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0.66546</td>\n",
       "      <td>0.66921</td>\n",
       "      <td>/nfs/turbo/umms-awaljee/wsliu/Data/NRD/y_pred_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1024</td>\n",
       "      <td>256</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.3</td>\n",
       "      <td>512</td>\n",
       "      <td>chf</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0.60173</td>\n",
       "      <td>0.60532</td>\n",
       "      <td>/nfs/turbo/umms-awaljee/wsliu/Data/NRD/y_pred_...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fc_width1  fc_width2      lr  dropout  batchsize cohort  tst_seed  n_fold  \\\n",
       "0       1024        256  0.0001      0.3        512    ami         0       5   \n",
       "1       1024        256  0.0001      0.3        512    chf         0       5   \n",
       "2       1024        256  0.0001      0.3        512    pna         0       5   \n",
       "3       1024        256  0.0001      0.3        512    ami         1       5   \n",
       "4       1024        256  0.0001      0.3        512    chf         1       5   \n",
       "\n",
       "   DX_rarecutpoint  PR_rarecutpoint  auc_mean  auc_avg  \\\n",
       "0               10               10   0.66806  0.67238   \n",
       "1               10               10   0.60358  0.60744   \n",
       "2               10               10   0.63760  0.64151   \n",
       "3               10               10   0.66546  0.66921   \n",
       "4               10               10   0.60173  0.60532   \n",
       "\n",
       "                                         y_pred_file  \n",
       "0  /nfs/turbo/umms-awaljee/wsliu/Data/NRD/y_pred_...  \n",
       "1  /nfs/turbo/umms-awaljee/wsliu/Data/NRD/y_pred_...  \n",
       "2  /nfs/turbo/umms-awaljee/wsliu/Data/NRD/y_pred_...  \n",
       "3  /nfs/turbo/umms-awaljee/wsliu/Data/NRD/y_pred_...  \n",
       "4  /nfs/turbo/umms-awaljee/wsliu/Data/NRD/y_pred_...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_grouped = res.groupby(['cohort'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">auc_mean</th>\n",
       "      <th colspan=\"2\" halign=\"left\">auc_avg</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cohort</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ami</th>\n",
       "      <td>0.706992</td>\n",
       "      <td>10</td>\n",
       "      <td>0.709505</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chf</th>\n",
       "      <td>0.622828</td>\n",
       "      <td>10</td>\n",
       "      <td>0.627054</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pna</th>\n",
       "      <td>0.663455</td>\n",
       "      <td>10</td>\n",
       "      <td>0.667562</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        auc_mean         auc_avg      \n",
       "            mean count      mean count\n",
       "cohort                                \n",
       "ami     0.706992    10  0.709505    10\n",
       "chf     0.622828    10  0.627054    10\n",
       "pna     0.663455    10  0.667562    10"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_grouped[['auc_mean', 'auc_avg']].agg(['mean', 'count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'multi_space_glove/'\n",
    "auc_lst = []\n",
    "ap_lst = []\n",
    "ap_avg_lst = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in res.iterrows():\n",
    "    y_pred_mat = np.load(row.y_pred_file)\n",
    "    y_pred = y_pred_mat[:, 0]\n",
    "    y_pred_avg = y_pred_mat.mean(axis=1)\n",
    "    cohort = row.cohort\n",
    "    tst_seed = row.tst_seed\n",
    "    all_df = pd.read_csv(path+folder+'cohorts10/{}/pred_comorb.csv'.format(cohort), dtype=core_dtypes_pd)\n",
    "    tst_key = pd.read_csv(path+folder+'cohorts10/{}/tst_key{}.csv'.format(cohort, tst_seed), names = ['KEY_NRD'])\n",
    "    tst_df = all_df.loc[all_df.KEY_NRD.isin(tst_key.KEY_NRD)]\n",
    "    y_true = tst_df.readm30.values\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_pred_avg)\n",
    "    auc_avg = auc(fpr, tpr)\n",
    "    ap = average_precision_score(y_true, y_pred)\n",
    "    ap_avg = average_precision_score(y_true, y_pred_avg)\n",
    "    auc_lst.append(auc_avg)\n",
    "    ap_lst.append(ap)\n",
    "    ap_avg_lst.append(ap_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = res.assign(auprc=ap_lst)\n",
    "res = res.assign(auprc_avg=ap_avg_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_grouped = res.groupby(['cohort'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ami mean: 0.707 (0.705, 0.709) avg: 0.710 (0.707, 0.712) auprc: 0.233 (0.228, 0.238) auprc_avg: 0.235 (0.229, 0.240)\n",
      "chf mean: 0.623 (0.620, 0.626) avg: 0.627 (0.624, 0.630) auprc: 0.256 (0.252, 0.259) auprc_avg: 0.258 (0.255, 0.262)\n",
      "pna mean: 0.663 (0.660, 0.666) avg: 0.668 (0.664, 0.671) auprc: 0.239 (0.235, 0.242) auprc_avg: 0.242 (0.238, 0.246)\n"
     ]
    }
   ],
   "source": [
    "for n, g in res_grouped:\n",
    "    print(n, 'mean: {0:.3f} ({1:.3f}, {2:.3f})'.format(g.auc_mean.mean(), *sms.DescrStatsW(g.auc_mean).zconfint_mean()), \n",
    "         'avg: {0:.3f} ({1:.3f}, {2:.3f})'.format(g.auc_avg.mean(), *sms.DescrStatsW(g.auc_avg).zconfint_mean()), \n",
    "         'auprc: {0:.3f} ({1:.3f}, {2:.3f})'.format(g.auprc.mean(), *sms.DescrStatsW(g.auprc).zconfint_mean()), \n",
    "         'auprc_avg: {0:.3f} ({1:.3f}, {2:.3f})'.format(g.auprc_avg.mean(), *sms.DescrStatsW(g.auprc_avg).zconfint_mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ami mean: 0.7138 (0.0016) avg: 0.7164 (0.0016)\n",
      "chf mean: 0.6252 (0.0013) avg: 0.6291 (0.0013)\n",
      "pna mean: 0.6675 (0.0012) avg: 0.6700 (0.0011)\n"
     ]
    }
   ],
   "source": [
    "for n, g in res_grouped:\n",
    "    print(n, 'mean: {0:.4f} ({1:.4f})'.format(g.auc_mean.mean(), g.auc_mean.std()/np.sqrt(len(g))), \n",
    "         'avg: {0:.4f} ({1:.4f})'.format(g.auc_avg.mean(), g.auc_avg.std()/np.sqrt(len(g))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.to_csv('output/ht_result0222.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
